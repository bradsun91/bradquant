{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "np.set_printoptions(suppress=True)# 关掉科学计数法\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "# 一次性merge多个pct_chg\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n",
    "from statsmodels import regression\n",
    "\n",
    "# import tushare as ts\n",
    "import time, urllib\n",
    "# ts.set_token('8ef5ec61cdd848715c57c11d58dd71da1271f76b2420d2bac8aef123')\n",
    "# pro = ts.pro_api('8ef5ec61cdd848715c57c11d58dd71da1271f76b2420d2bac8aef123')\n",
    "\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=False, rc=None)\n",
    "\n",
    "\n",
    "# from plotly.graph_objs import Scatter,Layout\n",
    "# import plotly\n",
    "# import plotly.offline as py\n",
    "# import numpy as np\n",
    "# import plotly.graph_objs as go\n",
    "\n",
    "# #setting offilne\n",
    "# plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "- signal_generators: merge market data with technical analysis signals\n",
    "- position calculators: calculate position based on a variety of principles (money-hedge/beta-hedge/risk-parity)\n",
    "- ticker filters:\n",
    "- backtester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TechnicalIndicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalIndicators(object):\n",
    "\n",
    "    def EMA(df, n, price_col): # n = 5\n",
    "        \"\"\"\n",
    "        Exponential Moving Average\n",
    "        rationale CHECKED, code CHECKED, updated.\n",
    "\n",
    "        params:\n",
    "            df: pd dataframe\n",
    "            n: number of days = 5\n",
    "        \"\"\"\n",
    "        EMA = df[price_col].ewm(span=n, min_periods=n - 1).mean().rename('EMA_' + str(n))\n",
    "        return EMA\n",
    "\n",
    "    def OBV(df, n, price_col, vol_col): # n = 5\n",
    "        \"\"\"On-balance Volume\n",
    "\n",
    "        On Balance Volume (OBV) measures buying and selling pressure as a cumulative indicator that adds \n",
    "        volume on up days and subtracts volume on down days. OBV was developed by Joe Granville and introduced \n",
    "        in his 1963 book, Granville's New Key to Stock Market Profits. It was one of the first indicators to \n",
    "        measure positive and negative volume flow. Chartists can look for divergences between OBV and price \n",
    "        to predict price movements or use OBV to confirm price trends.\n",
    "\n",
    "        http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:on_balance_volume_obv\n",
    "        \"\"\"\n",
    "        df = df.reset_index()\n",
    "        i = 0\n",
    "        OBV = [0]\n",
    "        while i < df.index[-1]:\n",
    "            if df.at[i + 1, price_col] - df.at[i, price_col] > 0:\n",
    "                OBV.append(df.at[i + 1, vol_col])\n",
    "            if df.at[i + 1, price_col] - df.at[i, price_col] == 0:\n",
    "                OBV.append(0)\n",
    "            if df.at[i + 1, price_col] - df.at[i, price_col] < 0:\n",
    "                OBV.append(-df.at[i + 1, vol_col])\n",
    "            i = i + 1\n",
    "        OBV = pd.Series(OBV)\n",
    "        OBV_ma = pd.Series(OBV.rolling(window=n).mean(), name = 'OBV_' + str(n))\n",
    "        return OBV_ma\n",
    "\n",
    "    # Rationale checked\n",
    "    def MFI(df, n, hi_col, lo_col, price_col, vol_col): # n = 14\n",
    "        \"\"\"Money Flow Index and Ratio, updated.\n",
    "        http://stockcharts.com/docs/doku.php?id=scans:indicators#money_flow_index_mfi\n",
    "\n",
    "        \"\"\"\n",
    "        df = df.reset_index()\n",
    "        PP = (df[hi_col] + df[lo_col] + df[price_col]) / 3\n",
    "        i  = 0\n",
    "        PosMF = [0]\n",
    "        while i < df.index[-1]:\n",
    "            if PP[i + 1] > PP[i]:\n",
    "                PosMF.append(PP[i + 1] * df.at[i + 1, vol_col])\n",
    "            else:\n",
    "                PosMF.append(0)\n",
    "            i = i + 1\n",
    "        PosMF = pd.Series(PosMF)\n",
    "        TotMF = PP * df[vol_col]\n",
    "        MFR   = pd.Series(PosMF / TotMF)\n",
    "        MFI   = pd.Series(MFR.rolling(window = n, center = False).mean(), name = 'MFI_' + str(n))\n",
    "        df    = df.join(MFI).set_index(\"index\")\n",
    "        return df[\"MFI_\" + str(n)]\n",
    "\n",
    "    # Done\n",
    "    # Rationale checked\n",
    "    def RSI(df, n, hi_col, lo_col): # n = 14\n",
    "        \"\"\"\n",
    "        Relative Strength Index, updated.\n",
    "        Conventional parameters: n = 14, 0.3 and 0.7 are two conventional thresholds\n",
    "        \"\"\"\n",
    "        df = df.reset_index()\n",
    "        i = 0\n",
    "        UpI = [0]\n",
    "        DoI = [0]\n",
    "        while i + 1 <= df.index[-1]:\n",
    "            UpMove = df.at[i + 1, hi_col] - df.at[i, hi_col]\n",
    "            DoMove = df.at[i, lo_col] - df.at[i + 1, lo_col]\n",
    "            if UpMove > DoMove and UpMove > 0:\n",
    "                UpD = UpMove\n",
    "            else: UpD = 0\n",
    "            UpI.append(UpD)\n",
    "            if DoMove > UpMove and DoMove > 0:\n",
    "                DoD = DoMove\n",
    "            else: DoD = 0\n",
    "            DoI.append(DoD)\n",
    "            i = i + 1\n",
    "        UpI   = pd.Series(UpI)\n",
    "        DoI   = pd.Series(DoI)\n",
    "        PosDI = UpI.ewm(span = n, min_periods = n - 1).mean()\n",
    "        NegDI = DoI.ewm(span = n, min_periods = n - 1).mean()\n",
    "        RSI   = pd.Series(PosDI / (PosDI + NegDI), name = 'RSI_' + str(n))\n",
    "        df    = df.join(RSI).set_index(\"index\")\n",
    "        return df[\"RSI_\" + str(n)]\n",
    "\n",
    "    def BIAS(df, n, price_col):\n",
    "        BIAS = df[price_col]-df[price_col].rolling(window=n).mean().rename('BIAS_'+str(n))\n",
    "        return BIAS\n",
    "\n",
    "    def MACD(df, n_fast, n_slow, n_macd, price_col): # n_fast = 12, n_slow = 26\n",
    "        \"\"\"\n",
    "        http://stockcharts.com/docs/doku.php?id=scans:indicators\n",
    "        MACD, MACD Signal and MACD difference, rationale CHECKED, code CHECKED, updated\n",
    "        # Conventional look-back window for calculating MACDsign is 9\n",
    "        \"\"\"\n",
    "        EMAfast = df[price_col].ewm(span = n_fast, min_periods = n_fast - 1).mean()\n",
    "        EMAslow = df[price_col].ewm(span = n_slow, min_periods = n_slow - 1).mean()\n",
    "        MACD = pd.Series(EMAfast - EMAslow, name = 'MACD_' + str(n_fast) + '_' + str(n_slow))\n",
    "        MACDsign = MACD.ewm(span = n_macd, min_periods = n_macd-1).mean().rename('MACDsign_' + str(n_fast) + '_' + str(n_slow))\n",
    "        MACDdiff = pd.Series(MACD - MACDsign, name = 'MACDdiff_' + str(n_fast) + '_' + str(n_slow))\n",
    "        df['MACD_Diff'] = MACD\n",
    "        df['MACD_Diff_EMA'] = MACDsign\n",
    "        df['MACD'] = MACDdiff\n",
    "        df['SIGNAL_STATUS'] = df['MACD'].apply(lambda x: \"多头状态\" if x>0 else (\"空头状态\" if x<0 else \"无信号状态\"))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeSeriesToolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData(object):\n",
    "    \n",
    "    def get_date_price_code_df(path, ticker_list, date_col, price_col, code_col):\n",
    "        # for etf data cols are 'date', 'close', 'code'\n",
    "        ticker_df_list = []\n",
    "#         print(ticker_list)\n",
    "        for ticker in ticker_list:\n",
    "            print(ticker)\n",
    "            try:\n",
    "#                 print(\"get thru\")\n",
    "                \n",
    "                ticker_df = pd.read_csv(path+ticker+\".csv\")\n",
    "                ticker_df[code_col] = ticker_df[code_col].astype(str)\n",
    "                ticker_df = ticker_df.sort_values(date_col)\n",
    "                ticker_df = ticker_df[[date_col, price_col, code_col]]\n",
    "#                 print(ticker_df)\n",
    "                ticker_df_list.append(ticker_df)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        try:\n",
    "            tickers_data_concated = pd.concat(ticker_df_list)\n",
    "            tickers_data_concated.reset_index(inplace=True)\n",
    "            del tickers_data_concated['index']  \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "#         print(tickers_data_concated)\n",
    "        return tickers_data_concated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesToolbox(object):\n",
    "        \n",
    "    def make_numeric_signals(series):\n",
    "        for item in series:\n",
    "            if item ==\"多\":\n",
    "                return 1\n",
    "            elif item ==\"空\":\n",
    "                return -1\n",
    "            else:\n",
    "                return 0 \n",
    "            \n",
    "    def merge_weights_and_signal(df_actions,\n",
    "                                 df_wts,\n",
    "                                 path,\n",
    "                                 code_col,\n",
    "                                 date_col,\n",
    "                                 price_col,\n",
    "                                 tgt_wts_mutiplier,\n",
    "                                 account_value):\n",
    "        if df_actions.empty:\n",
    "            print(\"There's no data in df_actions. No actional signals for today!\")\n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "            # 合并仓位数据和信号数据\n",
    "            df_actions_with_weights = df_wts.merge(df_actions, on =code_col)\n",
    "\n",
    "            # 仓位太小，创建2倍仓位信息, e.g. tgt_wts_mutiplier = 2\n",
    "            df_actions_with_weights['weight_enlarged'] = df_actions_with_weights['weight']*tgt_wts_mutiplier\n",
    "    #         print(df_actions_with_weights)\n",
    "            # 提取下一日要操作的tickers\n",
    "            tickers = list(df_actions_with_weights[code_col])\n",
    "    #         print(tickers)\n",
    "            # 得到这些tickers的收盘价数据\n",
    "    #         print(tickers)\n",
    "            tickers_closes = GetData.get_date_price_code_df(path,\n",
    "                                                             tickers,\n",
    "                                                             date_col, \n",
    "                                                             price_col, \n",
    "                                                             code_col)\n",
    "    #         print(tickers_closes)\n",
    "            # 创建今日date信息\n",
    "            last_date = tickers_closes[date_col].values[-1]\n",
    "    #         print(last_date)\n",
    "            # 提取最近一天的tickers的收盘价数据\n",
    "            tickers_closes_last_date = tickers_closes[tickers_closes[date_col] == last_date]\n",
    "\n",
    "            # 创建最终的信号-仓位指示信息\n",
    "            df_actions_with_weights = df_actions_with_weights.merge(tickers_closes_last_date, on = [date_col,\n",
    "                                                                                                    code_col])\n",
    "            df_actions_with_weights['tgt_shares'] = account_value*\\\n",
    "                                                    df_actions_with_weights['weight_enlarged']/\\\n",
    "                                                    df_actions_with_weights[price_col]\n",
    "            return df_actions_with_weights\n",
    "     \n",
    "    def merge_current_pos_with_target_pos(path, cur_positions, tgt_last_macd_signals):\n",
    "        tgt_last_macd_signals['TYPE'] = \"TARGET\"\n",
    "        # the following variables should be assigned first\n",
    "        cur_pos_macd = MACDSignals(path, \n",
    "                        cur_positions, \n",
    "                        date_col, \n",
    "                        code_col, \n",
    "                        price_col, \n",
    "                        n_fast, \n",
    "                        n_slow, \n",
    "                        n_macd, \n",
    "                        ticker_type)\n",
    "        cur_pos_macd_signals, \\\n",
    "        cur_pos_last_macd_signals, \\\n",
    "        cur_pos_df_actions = cur_pos_macd.calc_macd_signals()\n",
    "        cur_pos_last_macd_signals['TYPE'] = 'CUR_POS'\n",
    "        tgt_cur_macd_signal_df = cur_pos_last_macd_signals.merge(tgt_last_macd_signals, on = [date_col,code_col], how = 'outer')\n",
    "        return tgt_cur_macd_signal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PlotToolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotToolbox(object):\n",
    "    \n",
    "    def pie_graph(values, labels, pie_length, pie_width, title_name):\n",
    "        # draw pie graph\n",
    "        plt.figure(1, figsize = (pie_length, pie_width))\n",
    "        plt.axes(aspect=1)\n",
    "        plt.pie(x=values, labels=labels, autopct='%3.1f %%')\n",
    "        plt.title(title_name, fontsize = 15)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def plot_macd_signals(ticker, macd_signals, tail_num):\n",
    "        ticker_macd_signals = macd_signals[macd_signals[code_col]==ticker]\n",
    "        ticker_macd_signals.set_index(date_col, inplace = True)\n",
    "        ticker_macd_signals[[code_col,\"MACD\"]].tail(tail_num).plot(figsize = (15,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在分析环境里，筛选出tickers，然后使用MACD_signals\n",
    "\n",
    "class MACDSignals(object):\n",
    "    \n",
    "    def __init__(self, stocks_path, tickers, date_col, code_col, price_col, n_fast, n_slow, n_macd, ticker_type):\n",
    "        self.path = stocks_path\n",
    "        self.tickers = tickers\n",
    "        self.date_col = date_col\n",
    "        self.code_col = code_col\n",
    "        self.price_col = price_col\n",
    "        self.n_fast = n_fast\n",
    "        self.n_slow = n_slow\n",
    "        self.n_macd = n_macd\n",
    "        self.ticker_type = ticker_type\n",
    "        self.mkt_data = self.get_mkt_data_df()\n",
    "\n",
    "    def get_mkt_data_df(self):\n",
    "    # e.g. ch_db_path = \"/Users/miaoyuesun/Code_Workspace/brad_public_workspace_mac/data/CH_database/\"\n",
    "        csv_path = self.path+\"*.csv\"\n",
    "        files = glob.glob(csv_path)\n",
    "        ticker_df_list = []\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                ticker_df = pd.read_csv(self.path+ticker+\".csv\")\n",
    "                ticker_df[self.code_col] = ticker_df[self.code_col].astype(str)\n",
    "                ticker_df = ticker_df.sort_values(self.date_col)\n",
    "                ticker_df_list.append(ticker_df)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        try:\n",
    "            tickers_data_concated = pd.concat(ticker_df_list)\n",
    "            tickers_data_concated.reset_index(inplace=True)\n",
    "            del tickers_data_concated['index']  \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return tickers_data_concated\n",
    "    \n",
    "\n",
    "    def calc_macd_signals(self):\n",
    "        tickers_data_concated = self.mkt_data\n",
    "#         print(tickers_data_concated)\n",
    "        signal_record = []\n",
    "        signal_data = []\n",
    "        if len(self.tickers)!=1:\n",
    "            for ticker in self.tickers:\n",
    "                try:\n",
    "                    if self.ticker_type == \"float\":\n",
    "                        single_ticker_df = tickers_data_concated[tickers_data_concated[self.code_col]==float(ticker)]\n",
    "                    elif self.ticker_type == \"string\":\n",
    "                        single_ticker_df = tickers_data_concated[tickers_data_concated[self.code_col]==ticker]\n",
    "                        \n",
    "                    signal_df = TechnicalIndicators.MACD(single_ticker_df, self.n_fast, self.n_slow, self.n_macd, self.price_col)\n",
    "                    \n",
    "                    signal_data.append(signal_df)\n",
    "                except:\n",
    "                    pass\n",
    "            signal_data_df = pd.concat(signal_data)\n",
    "        else:\n",
    "            try:                \n",
    "                signal_df = TechnicalIndicators.MACD(single_ticker_df, self.n_fast, self.n_slow, self.n_macd, self.price_col)\n",
    "            except:\n",
    "                pass\n",
    "            signal_data_df = signal_df\n",
    "\n",
    "        # v1 is the version of generating the og macd signals\n",
    "        signal_data_df['SIGNAL_DIRECTION'] = signal_data_df['SIGNAL_STATUS'].apply(lambda x: TimeSeriesToolbox.make_numeric_signals(x))\n",
    "        signal_data_df['SIGNAL_DIRECTION_DIFF'] = signal_data_df.groupby([self.code_col])['SIGNAL_DIRECTION'].diff()\n",
    "        signal_data_df['SIGNAL_ACTION'] = signal_data_df['SIGNAL_DIRECTION_DIFF'].apply(lambda x: \"LONG\" if x==2 else(\"SHORT\" if x==-2 else \"NO CHANGE\"))\n",
    "#         print(signal_data_df)\n",
    "        most_recent_signals = signal_data_df.groupby([self.code_col])[[self.date_col,self.code_col,'SIGNAL_STATUS','SIGNAL_ACTION']].tail(1)\n",
    "        df_actions = most_recent_signals[most_recent_signals[\"SIGNAL_ACTION\"]!=\"NO CHANGE\"]\n",
    "        return signal_data_df, most_recent_signals, df_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "class RiskParity(object):\n",
    "    \n",
    "    def __init__(self, stocks_path, tickers, \n",
    "                 date_col, code_col, price_col, \n",
    "                 ticker_type, asset_name, draw_pie_graph):\n",
    "        \n",
    "        self.path = stocks_path\n",
    "        self.tickers = tickers\n",
    "        self.date_col = date_col\n",
    "        self.code_col = code_col\n",
    "        self.price_col = price_col\n",
    "        self.ticker_type = ticker_type\n",
    "        self.asset_name = asset_name\n",
    "        self.draw_pie_graph = draw_pie_graph\n",
    "        self.ticker_df_list = self.get_date_price_code_return_list()\n",
    "        self.tgt_returns = self.ticker_df_list\n",
    "        self.tgt_merged_returns = self.merge_dfs_by_ticker(self.tgt_returns, \n",
    "                                                           self.date_col)\n",
    "        self.wts, self.risk = self.get_smart_weight(self.tgt_merged_returns, \n",
    "                                                    method='risk parity', \n",
    "                                                    cov_adjusted=False, \n",
    "                                                    wts_adjusted=False)\n",
    "        self.df_wts, self.risk_parity_tickers, self.weights = self.get_df_wts()\n",
    "        \n",
    "        \n",
    "    # Get date_col, price_col, code_col, pct_chg_col\n",
    "    def get_date_price_code_return_list(self):\n",
    "        # for etf data cols are 'date', 'close', 'code'\n",
    "        ticker_df_list = []\n",
    "        for ticker in self.tickers:\n",
    "            try:\n",
    "                ticker_df = pd.read_csv(self.path+ticker+\".csv\")\n",
    "                ticker_df = ticker_df.sort_values(self.date_col)\n",
    "                ticker_df = ticker_df[[self.date_col, \n",
    "                                       self.price_col, \n",
    "                                       self.code_col]]\n",
    "                ticker_df['pct_chg'] = ticker_df[self.price_col].pct_change()\n",
    "                ticker_df = ticker_df[[self.date_col, 'pct_chg']].dropna()\n",
    "                ticker_df.columns = [self.date_col, ticker]\n",
    "                ticker_df_list.append(ticker_df)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        return ticker_df_list\n",
    "    \n",
    "    \n",
    "    def merge_dfs_by_ticker(self, ticker_df_list, date_col):\n",
    "        merged_all = reduce(lambda left, right: pd.merge(left, right, on=date_col), ticker_df_list)\n",
    "#         merged_all = reduce(merge_df_for_reduce, ticker_df_list)\n",
    "        merged_all.set_index(self.date_col, inplace=True)\n",
    "        merged_all.dropna(how=\"all\", axis = 1, inplace = True)\n",
    "        merged_all.fillna(method=\"ffill\", inplace = True)\n",
    "        return merged_all\n",
    "        \n",
    "        \n",
    "    def get_smart_weight(self, pct, method='risk parity', cov_adjusted=False, wts_adjusted=False):\n",
    "        if cov_adjusted == False:\n",
    "            #协方差矩阵\n",
    "            cov_mat = pct.cov()\n",
    "        else:\n",
    "            #调整后的半衰协方差矩阵\n",
    "            cov_mat = pct.iloc[:len(pct)/4].cov()*(1/10.) + pct.iloc[len(pct)/4+1:len(pct)/2].cov()*(2/10.) +\\\n",
    "                pct.iloc[len(pct)/2+1:len(pct)/4*3].cov()*(3/10.) + pct.iloc[len(pct)/4*3+1:].cov()*(4/10.)\n",
    "        if not isinstance(cov_mat, pd.DataFrame):\n",
    "            raise ValueError('cov_mat should be pandas DataFrame！')\n",
    "\n",
    "        omega = np.matrix(cov_mat.values)  # 协方差矩阵\n",
    "\n",
    "        a, b = np.linalg.eig(np.array(cov_mat)) #a为特征值,b为特征向量\n",
    "        a = np.matrix(a)\n",
    "        b = np.matrix(b)\n",
    "        # 定义目标函数\n",
    "    \n",
    "        def fun1(x):\n",
    "            tmp = (omega * np.matrix(x).T).A1\n",
    "            risk = x * tmp/ np.sqrt(np.matrix(x) * omega * np.matrix(x).T).A1[0]\n",
    "            delta_risk = [sum((i - risk)**2) for i in risk]\n",
    "            return sum(delta_risk)\n",
    "\n",
    "        def fun2(x):\n",
    "            tmp = (b**(-1) * omega * np.matrix(x).T).A1\n",
    "            risk = (b**(-1)*np.matrix(x).T).A1 * tmp/ np.sqrt(np.matrix(x) * omega * np.matrix(x).T).A1[0]\n",
    "            delta_risk = [sum((i - risk)**2) for i in risk]\n",
    "            return sum(delta_risk)\n",
    "    \n",
    "        # 初始值 + 约束条件 \n",
    "        x0 = np.ones(omega.shape[0]) / omega.shape[0]  \n",
    "        bnds = tuple((0,None) for x in x0)\n",
    "        cons = ({'type':'eq', 'fun': lambda x: sum(x) - 1})\n",
    "        options={'disp':False, 'maxiter':1000, 'ftol':1e-20}\n",
    "\n",
    "        if method == 'risk parity':\n",
    "            res = minimize(fun1, x0, bounds=bnds, constraints=cons, method='SLSQP', options=options)\n",
    "        elif method == 'pc risk parity':\n",
    "            res = minimize(fun2, x0, bounds=bnds, constraints=cons, method='SLSQP', options=options)\n",
    "        else:\n",
    "            raise ValueError('method error！！！')\n",
    "\n",
    "        # 权重调整\n",
    "        if res['success'] == False:\n",
    "            # print res['message']\n",
    "            pass\n",
    "        wts = pd.Series(index=cov_mat.index, data=res['x'])\n",
    "\n",
    "        if wts_adjusted == True:\n",
    "            wts[wts < 0.0001]=0.0\n",
    "            wts = wts / wts.sum()\n",
    "        elif wts_adjusted == False:\n",
    "            wts = wts / wts.sum()\n",
    "        else:\n",
    "            raise ValueError('wts_adjusted should be True/False！')\n",
    "\n",
    "        risk = pd.Series(wts * (omega * np.matrix(wts).T).A1 / np.sqrt(np.matrix(wts) * omega * np.matrix(wts).T).A1[0],index = cov_mat.index)\n",
    "        risk[risk<0.0] = 0.0\n",
    "        return wts,risk\n",
    "    \n",
    "        \n",
    "    def get_df_wts(self):\n",
    "        df_wts = pd.DataFrame(self.wts)\n",
    "        df_wts.reset_index(inplace = True)\n",
    "        df_wts.columns = [self.asset_name, 'weight']\n",
    "        risk_parity_tickers = list(df_wts[self.asset_name])\n",
    "        weights = list(df_wts['weight'])\n",
    "        return df_wts, risk_parity_tickers, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mannually check our current position holdings for potential exit signals:\n",
    "cur_positions = [\n",
    "#     \"510180\",#180ETF\n",
    "#     \"510300\",#300ETF\n",
    "#     \"510810\",#上海国企\n",
    "#     \"510850\",#工银上50\n",
    "#     \"510880\",#红利ETF\n",
    "#     '512000', #券商ETF -\n",
    "#     '512010',#医药ETF\n",
    "#     \"512400\",#有色ETF\n",
    "    \"512660\",#军工ETF\n",
    "#     '512690',#酒ETF\n",
    "#     '512800',#银行ETF\n",
    "#     '512880',#证券ETF -\n",
    "#     '159928',#消费ETF\n",
    "#     '512290',#生物医药ETF -\n",
    "#     '513050',#中概互联网\n",
    "#     '513100',#纳指ETF\n",
    "    '518880',#黄金ETF\n",
    "#     \"159905\",#深红利\n",
    "#     \"159920\",#恒生ETF\n",
    "#     \"159959\", #央企ETF -\n",
    "#     \"159939\"# 信息技术\n",
    "    '159938',#医药 -\n",
    "#     ''#券商ETF\n",
    "#     '512960',#央调ETF\n",
    "#     ''#证券ETF\n",
    "    '513500'#标普500\n",
    "#     \"513100\"#纳指\n",
    "    \n",
    "]\n",
    "len(cur_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns = {\n",
    "#     \"2020-03-02\":0,\n",
    "#     \"2020-03-03\":0,\n",
    "#     \"2020-03-04\":0,\n",
    "#     \"2020-03-05\":0,\n",
    "#     \"2020-03-06\":0,\n",
    "#     \"2020-03-09\":-0.21,\n",
    "#     \"2020-03-10\":0.04,\n",
    "#     \"2020-03-11\":-0.08,\n",
    "#     \"2020-03-12\":-0.21,\n",
    "#     \"2020-03-13\":-0.1,\n",
    "#     \"2020-03-16\":0,\n",
    "#     \"2020-03-17\":0,\n",
    "#     \"2020-03-18\":0,\n",
    "#     \"2020-03-19\":0,\n",
    "#     \"2020-03-20\":0.01,\n",
    "#     \"2020-03-23\":0,\n",
    "#     \"2020-03-24\":0,\n",
    "#     \"2020-03-25\":-0.01,\n",
    "#     \"2020-03-26\":-0.02,\n",
    "#     \"2020-03-27\":-0.41,\n",
    "#     \"2020-03-30\":-0.39,\n",
    "#     \"2020-03-31\":0.12,\n",
    "#     \"2020-04-01\":-0.61,\n",
    "#     \"2020-04-02\":0.82,\n",
    "#     \"2020-04-03\":-0.18,\n",
    "#     \"2020-04-07\":1.74,\n",
    "#     \"2020-04-08\":-0.33,\n",
    "#     \"2020-04-09\":0.42,\n",
    "#     \"2020-04-10\":\n",
    "\n",
    "# }\n",
    "# returns_df = pd.DataFrame(pd.Series(returns), columns=['daily_rtrn'])\n",
    "# returns_df = returns_df.reset_index().rename(columns = {\"index\":\"date\"})\n",
    "# returns_df['daily_rtrn'] = returns_df['daily_rtrn']/100\n",
    "# returns_df['sum_rtrn'] = returns_df['daily_rtrn'].cumsum()\n",
    "# returns_df['equity_rtrn'] = returns_df['sum_rtrn']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark's total share returns: 1111.111111111111\n",
      "Brad's total share returns: 444.4444444444444\n",
      "Kevin's total share returns: 444.4444444444444\n"
     ]
    }
   ],
   "source": [
    "print(\"Mark's total share returns:\",2000*(5/9))\n",
    "print(\"Brad's total share returns:\",2000*(2/9))\n",
    "print(\"Kevin's total share returns:\",2000*(2/9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miaoyuesun/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/miaoyuesun/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/miaoyuesun/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/miaoyuesun/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "failed in converting 4th argument `xl' of _slsqp.slsqp to C/Fortran array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-60e6f4a3ef76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                               \u001b[0mticker_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                               \u001b[0masset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                               True)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdf_wts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrisk_parity_tickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrisk_parity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_df_wts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8ac2fa837399>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stocks_path, tickers, date_col, code_col, price_col, ticker_type, asset_name, draw_pie_graph)\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'risk parity'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                     \u001b[0mcov_adjusted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                                     wts_adjusted=False)\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_wts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrisk_parity_tickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_df_wts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8ac2fa837399>\u001b[0m in \u001b[0;36mget_smart_weight\u001b[0;34m(self, pct, method, cov_adjusted, wts_adjusted)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'risk parity'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SLSQP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pc risk parity'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SLSQP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'slsqp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         return _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 618\u001b[0;31m                                constraints, callback=callback, **options)\n\u001b[0m\u001b[1;32m    619\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trust-constr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         return _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/scipy/optimize/slsqp.py\u001b[0m in \u001b[0;36m_minimize_slsqp\u001b[0;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, **unknown_options)\u001b[0m\n\u001b[1;32m    447\u001b[0m               \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m               \u001b[0miexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mireset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitermx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m               n1, n2, n3)\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;31m# call callback if major iteration has incremented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: failed in converting 4th argument `xl' of _slsqp.slsqp to C/Fortran array"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    stocks_path = \"/Users/miaoyuesun/Code_Workspace/brad_public_workspace_mac/data/CH_database/\"\n",
    "    etfs_df = pd.read_csv(\"filtered_50_etfs_by_vol_20200224.csv\")\n",
    "    tickers = list(etfs_df['etf'].str.split(\".\",expand = True)[0])\n",
    "#     date_col = 'trade_date'\n",
    "#     code_col = 'ts_code'\n",
    "#     price_col = 'close'\n",
    "    date_col = 'date'\n",
    "    code_col = 'code'\n",
    "    price_col = 'close'\n",
    "    ticker_type = 'string'\n",
    "    asset_name = \"code\"\n",
    "    tgt_wts_mutiplier = 4\n",
    "    account_value = 100000\n",
    "    \n",
    "#     # ========For calculating MACD signals========\n",
    "    n_fast = 12\n",
    "    n_slow = 26\n",
    "    n_macd = 9\n",
    "    macd = MACDSignals(stocks_path, \n",
    "                        tickers, \n",
    "                        date_col, \n",
    "                        code_col, \n",
    "                        price_col, \n",
    "                        n_fast, \n",
    "                        n_slow, \n",
    "                        n_macd, \n",
    "                        ticker_type)\n",
    "    macd_signals, last_macd_signals, df_actions = macd.calc_macd_signals()\n",
    "\n",
    "    # ========For calculating risk-parity weights========\n",
    "    risk_parity = RiskParity(stocks_path,\n",
    "                              tickers,\n",
    "                              date_col,\n",
    "                              code_col,\n",
    "                              price_col,\n",
    "                              ticker_type,\n",
    "                              asset_name,\n",
    "                              True)\n",
    "    \n",
    "    df_wts, risk_parity_tickers, weights = risk_parity.get_df_wts()\n",
    "    \n",
    "    df_actions_with_weights = TimeSeriesToolbox.merge_weights_and_signal(df_actions,\n",
    "                                                                     df_wts,\n",
    "                                                                     stocks_path,\n",
    "                                                                     code_col,\n",
    "                                                                     date_col,\n",
    "                                                                     price_col,\n",
    "                                                                     tgt_wts_mutiplier,\n",
    "                                                                     account_value)\n",
    "#     PlotToolbox.pie_graph(weights, risk_parity_tickers, 8, 8, \"Risk Parity Allocation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_cur_pos_df = TimeSeriesToolbox.merge_current_pos_with_target_pos(stocks_path, cur_positions, last_macd_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reentry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_recent_goldcross_close(close, signal_diff):\n",
    "    if signal_diff == 1 or signal_diff == 2:\n",
    "        return close\n",
    "\n",
    "macd_signals['recent_goldcross'] = macd_signals.apply(lambda row: find_recent_goldcross_close(row['close'],row['SIGNAL_DIRECTION_DIFF']), axis=1)\n",
    "\n",
    "# forward-fill all NaNs for the recent_goldcross to pave way for calculating the pct_diff between current \n",
    "# close and the rencent goldcross close\n",
    "macd_signals['recent_goldcross_ffill'] = macd_signals.groupby(\"code\")['recent_goldcross'].apply(lambda x: x.fillna(method=\"ffill\"))\n",
    "macd_signals['pctchg_from_recent_goldcrossclose'] = (macd_signals['close']-macd_signals['recent_goldcross_ffill'])/macd_signals['recent_goldcross_ffill']\n",
    "macd_signals['reentry_rank'] = macd_signals.groupby([\"date\"])['pctchg_from_recent_goldcrossclose'].rank(ascending = True)\n",
    "last_day = macd_signals['date'].values[-1]\n",
    "macd_signals_reentry = macd_signals[macd_signals['date']==last_day][macd_signals['pctchg_from_recent_goldcrossclose']<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reentry_tickers = list(macd_signals_reentry['code'])\n",
    "tickers_to_reenter = []\n",
    "for ticker in all_reentry_tickers:\n",
    "    if ticker not in cur_positions:\n",
    "        tickers_to_reenter.append(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_to_reenter_df = pd.DataFrame(tickers_to_reenter, columns=['code'])\n",
    "df_actions_with_weights_reentry = df_wts.merge(tickers_to_reenter_df, on = ['code'])\n",
    "macd_signals_reentry = macd_signals[macd_signals['date']==last_day]\n",
    "df_actions_with_weights_reentry = df_actions_with_weights_reentry.merge(macd_signals_reentry, on = 'code')\n",
    "df_actions_with_weights_reentry['weight_enlarged'] = df_actions_with_weights_reentry['weight']*tgt_wts_mutiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actions_with_weights_reentry['tgt_shares'] = account_value*\\\n",
    "                                                    df_actions_with_weights_reentry['weight_enlarged']/\\\n",
    "                                                    df_actions_with_weights_reentry[price_col]\n",
    "\n",
    "df_actions_with_weights_reentry = df_actions_with_weights_reentry[['code','date','SIGNAL_STATUS','SIGNAL_ACTION',\\\n",
    "                                                                   'weight_enlarged','close','tgt_shares',\\\n",
    "                                                                   'pctchg_from_recent_goldcrossclose','reentry_rank']]\n",
    "df_actions_with_weights_reentry.sort_values('reentry_rank', ascending=True, inplace=True)\n",
    "df_actions_with_weights_reentry_L = df_actions_with_weights_reentry[df_actions_with_weights_reentry['SIGNAL_STATUS']==\"多头状态\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actions_with_weights_reentry_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_reentry_tickers = list(df_actions_with_weights_reentry_L['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_reentry_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Current Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show if there are any actions needed to be made for our current positions\n",
    "tgt_cur_pos_df[tgt_cur_pos_df['TYPE_x']==\"CUR_POS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9228.5-369.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if there's new positions to be entered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any new positions to be entered\n",
    "tgt_cur_pos_df[(tgt_cur_pos_df['TYPE_x']!=\"CUR_POS\")&(tgt_cur_pos_df['SIGNAL_ACTION_y']!=\"NO CHANGE\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_actions_with_weights = df_actions_with_weights[df_actions_with_weights['SIGNAL_ACTION']=='LONG']\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_actions_tickers = list(df_actions_with_weights['code'])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actions_with_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_list_to_backtest = df_actions_tickers+selected_reentry_tickers\n",
    "symbol_list_to_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(symbol_list_to_backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actions_with_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1500*3.910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CH_backtest import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_performances = {}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for ticker in symbol_list_to_backtest:\n",
    "    # csv_dir = REPLACE_WITH_YOUR_CSV_DIR_HERE\n",
    "        equity_folder = \"./\"\n",
    "        csv_dir = \"/Users/miaoyuesun/Code_Workspace/brad_public_workspace_mac/data/CH_database/\"\n",
    "#         data_cols = ['trade_date', 'open', 'high','low', 'close', 'volume','ts_code'] #for ETFs # 要改代码\n",
    "#         data_cols = ['ts_code','trade_date','open','high','low','close','pre_close','change','pct_chg','vol','amount'] #for stocks\n",
    "#         commissions = 5 # RMB/USD per trade #要改代码\n",
    "        initial_capital = 1000000.0\n",
    "        start_date = datetime.datetime(1991,12,1,0,0,0)\n",
    "        start_date_str = str(start_date)\n",
    "        heartbeat = 0.0\n",
    "#         price_col = \"close\" #要改代码\n",
    "#         qty = 5000 # 要改代码\n",
    "        backtest = Backtest(csv_dir, \n",
    "                            [ticker], \n",
    "                            initial_capital, \n",
    "                            heartbeat,\n",
    "                            start_date,\n",
    "                            HistoricCSVDataHandler, \n",
    "                            SimulatedExecutionHandler, \n",
    "                            Portfolio, \n",
    "    #                         MovingAverageCrossStrategy,\n",
    "                           MovingAverageConvergenceDivergence)\n",
    "\n",
    "        backtest.simulate_trading()\n",
    "        df_equity = pd.read_csv(ticker+\"_performance\"+\".csv\")\n",
    "        df_equity.drop_duplicates(\"datetime\", inplace = True)\n",
    "        df_equity =df_equity[df_equity['datetime']>start_date_str]\n",
    "        df_equity.index = df_equity['datetime']\n",
    "        df_equity = df_equity[df_equity['total'].map(lambda x: str(x)!=\"nan\")]\n",
    "        df_equity.columns = ['datetime', 'market_value', 'cash', 'commission', 'total', 'returns',\n",
    "        'equity_curve', 'drawdown']\n",
    "        df_equity_copy = df_equity.copy()\n",
    "        data = df_equity_copy\n",
    "#         win_rate, mean_win_loss_ratio, bt_score, profits = performance(data)\n",
    "        single_stats = performance(data)\n",
    "#         print(\"TICKER: \", ticker)\n",
    "        ticker_performances[ticker] = single_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ticker_performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Signal Status on Long/Shorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MACD_long_short_counts(macd_signals, date_col, code_col):\n",
    "    long_short_counts = macd_signals.groupby([date_col,'SIGNAL_DIRECTION'])[code_col].count()\n",
    "    long_short_counts_df = pd.DataFrame(long_short_counts)\n",
    "    long_short_counts_df.reset_index(inplace=True)\n",
    "    long_counts_df = long_short_counts_df[long_short_counts_df['SIGNAL_DIRECTION']==1]\n",
    "    short_counts_df = long_short_counts_df[long_short_counts_df['SIGNAL_DIRECTION']==-1]\n",
    "    long_short_counts_df_merged = long_counts_df.merge(short_counts_df, on = date_col)\n",
    "    long_short_counts_df_merged['long_short_ratio'] = long_short_counts_df_merged['code_x']/(\\\n",
    "                                                                            long_short_counts_df_merged['code_x']+\\\n",
    "                                                                            long_short_counts_df_merged['code_y'])\n",
    "    long_short_counts_df_merged.index = pd.to_datetime(long_short_counts_df_merged[date_col])\n",
    "    return long_short_counts_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macd_long_short_counts = MACD_long_short_counts(macd_signals, date_col, code_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macd_long_short_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = macd_long_short_counts[date_col].values[-1]\n",
    "macd_long_short_counts[['code_x','code_y']].tail(100).plot(figsize = (18,10));\n",
    "plt.title(\"Daily Number of Long Signals vs Short Signals for 50 ETFs by {}\".format(today), fontsize = 17);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macd_long_short_counts['long_short_ratio'].tail(100).plot(figsize = (18,10))\n",
    "long_short_ratio = macd_long_short_counts['long_short_ratio'].values[-1]\n",
    "plt.title(\"Long_Short_Ratio: {}, by {}\".format(round(long_short_ratio,2), today), fontsize =15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在分析环境里，筛选出tickers，然后使用MACD_signals\n",
    "\n",
    "class BIAS_signals(object):\n",
    "    \n",
    "    def __init__(self, stocks_path, tickers, date_col, code_col, price_col, n_fast, n_slow, n_macd, ticker_type):\n",
    "        self.path = stocks_path\n",
    "        self.tickers = tickers\n",
    "        self.date_col = date_col\n",
    "        self.code_col = code_col\n",
    "        self.price_col = price_col\n",
    "        self.ticker_type = ticker_type\n",
    "    \n",
    "\n",
    "    def get_mkt_data_df(self):\n",
    "    # e.g. ch_db_path = \"/Users/miaoyuesun/Code_Workspace/brad_public_workspace_mac/data/CH_database/\"\n",
    "        print(self.path)\n",
    "        csv_path = self.path+\"*.csv\"\n",
    "        files = glob.glob(csv_path)\n",
    "        ticker_df_list = []\n",
    "        for ticker in self.tickers:\n",
    "            try:\n",
    "                ticker_df = pd.read_csv(self.path+ticker+\".csv\")\n",
    "                ticker_df = ticker_df.sort_values(self.date_col)\n",
    "                ticker_df_list.append(ticker_df)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        try:\n",
    "            tickers_data_concated = pd.concat(ticker_df_list)\n",
    "            tickers_data_concated.reset_index(inplace=True)\n",
    "            del tickers_data_concated['index']  \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return tickers_data_concated\n",
    "    \n",
    "\n",
    "    def calc_macd_signals(self):\n",
    "        tickers_data_concated = get_mkt_data_df(self.path, self.tickers, self.date_col)\n",
    "        signal_record = []\n",
    "        signal_data = []\n",
    "        if len(self.tickers)!=1:\n",
    "            for ticker in self.tickers:\n",
    "                try:\n",
    "                    if self.ticker_type == \"float\":\n",
    "                        single_ticker_df = tickers_data_concated[tickers_data_concated[self.code_col]==float(ticker)]\n",
    "                    elif self.ticker_type == \"string\":\n",
    "                        single_ticker_df = tickers_data_concated[tickers_data_concated[self.code_col]==ticker]\n",
    "                        \n",
    "                    signal_df = MACD(single_ticker_df, self.n_fast, self.n_slow, self.n_macd, self.price_col)# 这个地方出错了\n",
    "                    signal_data.append(signal_df)\n",
    "                except:\n",
    "                    pass\n",
    "            signal_data_df = pd.concat(signal_data)\n",
    "        else:\n",
    "            try:                \n",
    "                signal_df = MACD(single_ticker_df, self.n_fast, self.n_slow, self.n_macd, self.price_col)\n",
    "            except:\n",
    "                pass\n",
    "            signal_data_df = signal_df\n",
    "\n",
    "        # v1 is the version of generating the og macd signals\n",
    "        signal_data_df['SIGNAL_DIRECTION'] = signal_data_df['SIGNAL_STATUS'].apply(lambda x: make_numeric_signals(x))\n",
    "        signal_data_df['SIGNAL_DIRECTION_DIFF'] = signal_data_df.groupby([self.code_col])['SIGNAL_DIRECTION'].diff()\n",
    "        signal_data_df['SIGNAL_ACTION'] = signal_data_df['SIGNAL_DIRECTION_DIFF'].apply(lambda x: \"LONG\" if x==2 else(\"SHORT\" if x==-2 else \"NO CHANGE\"))\n",
    "        most_recent_signals = signal_data_df.groupby([self.code_col])[[self.date_col,self.code_col,'SIGNAL_STATUS','SIGNAL_ACTION']].tail(1)\n",
    "        return signal_data_df, most_recent_signals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
