{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from datetime import datetime\n",
    "pd.set_option('max_colwidth',200)\n",
    "# import yfinance as yf\n",
    "import time, urllib\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tushare as ts\n",
    "ts.set_token('8ef5ec61cdd848715c57c11d58dd71da1271f76b2420d2bac8aef123')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tushare Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取个股"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = ts.pro_api()\n",
    "\n",
    "#获取单一股票行情\n",
    "df = pro.us_daily(ts_code='AAPL', start_date='20190101', end_date='20190904')\n",
    "\n",
    "#获取某一日所有股票\n",
    "df = pro.us_daily(trade_date='20190904')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('ts_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current list of S&P 500 from Wikipedia Api\n",
    "def call_sp500_list(vpn):\n",
    "    if vpn==1:\n",
    "        table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "        df = table[0]\n",
    "        list_ = pd.concat([df.Symbol,df.loc[:, 'GICS Sector']], axis=1)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        list_.to_csv(\"most_recent_sp500_list.csv\", index = False)\n",
    "#         print(list_)\n",
    "    elif vpn==0:\n",
    "        list_ = pd.read_csv(\"most_recent_sp500_list.csv\")\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = call_sp500_list(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 按照日期循环取股票数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "抱歉，您每天最多访问该接口5次，权限的具体详情访问：https://tushare.pro/document/1?doc_id=108。",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d87ecdbfa591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpro_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mus_daily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AAPL'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'BA'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'20190101'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'20190904'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tushare/pro/client.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, api_name, fields, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'msg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fields'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: 抱歉，您每天最多访问该接口5次，权限的具体详情访问：https://tushare.pro/document/1?doc_id=108。"
     ]
    }
   ],
   "source": [
    "pro = ts.pro_api()\n",
    "df = pro.us_daily(ts_code=['AAPL','BA'], start_date='20190101', end_date='20190904')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_weather():\n",
    "    all_weather_portfolio = [\"VTI\",\"TLT\",\"IEF\",\"GLD\",\"DBC\",\"SHY\",\"IEI\"]\n",
    "    return all_weather_portfolio\n",
    "\n",
    "def us_sectors_etf():\n",
    "    us_sectors = [\"IYM\",\"IYZ\",\"FCL\",\"FCD\",\"IYE\",\"IYG\",\"IYH\",\"IYJ\",\"IYR\",\"IYW\",\"IDU\"]\n",
    "    return us_sectors\n",
    "    \n",
    "def today_dt():\n",
    "    today = str(datetime.now().date())\n",
    "    return today\n",
    "\n",
    "def download_yf_data(start, end, us_db_path, ticker_list):\n",
    "    for ticker in ticker_list:\n",
    "    #     print(\"testing\")\n",
    "        # Initial downloading:\n",
    "        if not os.path.exists(us_db_path+ticker+\".csv\"):\n",
    "            print(\"{} is new, start downloading now...\".format(ticker))\n",
    "            with eventlet.Timeout(60,False):\n",
    "                try:\n",
    "                    data = yf.download(ticker, start=start, end=end)\n",
    "                    data.reset_index(inplace = True)\n",
    "                    data['Ticker'] = ticker\n",
    "                    data.to_csv(us_db_path+ticker+\".csv\", index = False)\n",
    "                    print(\"{} data file created: {}\".format(ticker, end))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    #         print(\"Timed Out: Download Failed!\")\n",
    "        # Check for updates:\n",
    "        else:\n",
    "            print(\"Already have data csv for {}\".format(ticker))\n",
    "            hist_data = pd.read_csv(us_db_path+ticker+\".csv\")\n",
    "            try:\n",
    "                hist_data_first_date = hist_data['Date'].values[0]\n",
    "                if start >= hist_data_first_date:\n",
    "                    hist_data_last_date = hist_data['Date'].values[-1]\n",
    "                    if today > hist_data_last_date:\n",
    "                        print(\"Needs to update, start updating new data for {} now...\".format(ticker))\n",
    "                        update_start = hist_data_last_date\n",
    "                        update_end = today\n",
    "                        with eventlet.Timeout(60,False):\n",
    "                            try:\n",
    "                                new_data = yf.download(ticker, start=update_start, end=update_end)\n",
    "                                new_data.reset_index(inplace = True)\n",
    "                                new_data['Ticker'] = ticker\n",
    "                                new_data.to_csv(us_db_path+ticker+\".csv\", mode='a', header=False, index = False)\n",
    "                                updated_duplicated_df = pd.read_csv(us_db_path+ticker+\".csv\")\n",
    "                                updated_df = updated_duplicated_df.drop_duplicates(\"Date\")\n",
    "                                updated_df.sort_values(\"Date\", inplace = True)\n",
    "                                updated_df.to_csv(us_db_path+ticker+\".csv\", index = False)\n",
    "                                print(\"New data updated till today for {}!\".format(ticker))\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "            #             print(\"Timed Out: Update Failed!\")\n",
    "                    else:\n",
    "                        print(\"There's no new data to update for {}.\".format(ticker))\n",
    "\n",
    "                else:\n",
    "                    print(\"Setup start date earlier than existing data's, trying to pull data from before...\")\n",
    "                    hist_data_last_date = hist_data['Date'].values[-1]\n",
    "                    if today > hist_data_last_date:\n",
    "                        print(\"Needs to update, start updating new data for {} now...\".format(ticker))\n",
    "                        update_start = start\n",
    "                        update_end = today\n",
    "                        with eventlet.Timeout(60,False):\n",
    "                            try:\n",
    "                                new_data = yf.download(ticker, start=update_start, end=update_end)\n",
    "                                new_data.reset_index(inplace = True)\n",
    "                                new_date = new_data['Date'].values[0]\n",
    "                                print(\"New data's first pulled date is {}\".format(new_date))\n",
    "                                new_data['Ticker'] = ticker\n",
    "\n",
    "                                new_data.to_csv(us_db_path+ticker+\".csv\", mode='a', header=False, index = False)\n",
    "                                updated_duplicated_df = pd.read_csv(us_db_path+ticker+\".csv\")\n",
    "                                updated_df = updated_duplicated_df.drop_duplicates(\"Date\")\n",
    "                                updated_df.sort_values(\"Date\", inplace = True)\n",
    "                                updated_df.to_csv(us_db_path+ticker+\".csv\", index = False)\n",
    "                                print(\"New data updated till today for {}!\".format(ticker))\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "                    else:\n",
    "                        print(\"There's no new data to update for {}.\".format(ticker))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    #     print(\"Data Download/Update for {} is Finished.\".format(ticker))\n",
    "        print(\"===============================================\")\n",
    "    print(\"【Updated Finished for today!】\")\n",
    "\n",
    "\n",
    "start = \"2005-01-01\"\n",
    "today = today_dt()\n",
    "end = today\n",
    "us_db_path = \"/Users/miaoyuesun/Code_Workspace/brad_public_workspace_mac/data/US_database/\"\n",
    "mkt_etf = [\"SPY\",\"VXXB\",\"QQQ\",\"VXX\",\"^VIX\"]\n",
    "other_tickers = [\"YELP\",'UBER','TSLA']\n",
    "us_sectors = us_sectors_etf()\n",
    "all_weather = all_weather()\n",
    "data_path = \"/Users/miaoyuesun/Code_Workspace/brad_public_workspace_mac/quant_research/data_pipeline/\"\n",
    "tickers_df = pd.read_csv(data_path+\"most_recent_sp500_tickers.csv\")\n",
    "sp500_tickers = list(tickers_df['sp500_tickers'])\n",
    "ticker_list = sp500_tickers+all_weather+us_sectors+mkt_etf+other_tickers\n",
    "# ticker_list = ['ROKU']\n",
    "\n",
    "import eventlet\n",
    "eventlet.monkey_patch()\n",
    "\n",
    "download_yf_data(start, end, us_db_path, ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
