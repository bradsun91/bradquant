{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "location = \"C:/Users/workspace/brad_public_workspace_on_win/SH_tongliang/data/\"\n",
    "file_name = \"11_13_ML_BTC_daily_df_use_this.csv\"\n",
    "\n",
    "rnn_unit = 10  # 隐层数量\n",
    "input_size = 7\n",
    "output_size = 1\n",
    "lr = 0.0006  # 学习率\n",
    "#——————————————————导入数据——————————————————————\n",
    "f = open(location+file_name)\n",
    "df = pd.read_csv(f)  # 读入股票数据\n",
    "data = df.iloc[:, 2:10].values  # 取第3-10列\n",
    "range_ = 10\n",
    "# print (len(data))\n",
    "\n",
    "# 获取训练集，train_end由原来的5800改成了500.batch_size改成了10\n",
    "def get_train_data(batch_size=10, time_step=20, train_begin=0, train_end=500):\n",
    "    batch_index = []\n",
    "    data_train = data[train_begin:train_end]\n",
    "    normalized_train_data = (\n",
    "        data_train - np.mean(data_train, axis=0)) / np.std(data_train, axis=0)  # 标准化\n",
    "    train_x, train_y = [], []  # 训练集\n",
    "    for i in range(len(normalized_train_data) - time_step):\n",
    "        if i % batch_size == 0:\n",
    "            batch_index.append(i)\n",
    "        x = normalized_train_data[i:i + time_step, :7]\n",
    "        y = normalized_train_data[i:i + time_step, 7, np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    batch_index.append((len(normalized_train_data) - time_step))\n",
    "    # print (\"train_x: \", train_x)# Brad新加, train_x data有数值\n",
    "    # print (\"train_y: \", train_y)# Brad新加，train_y data有数值\n",
    "    return batch_index, train_x, train_y\n",
    "\n",
    "\n",
    "# 获取测试集，test_begin改成了100\n",
    "def get_test_data(time_step=20, test_begin=500):\n",
    "    data_test = data[test_begin:1000]\n",
    "    print (\"data_test: \", data_test)\n",
    "    mean = np.mean(data_test, axis=0)\n",
    "    std = np.std(data_test, axis=0)\n",
    "    # print (\"mean: \", mean)# Brad新加的,all nan\n",
    "    # print (\"std\", std)# Brad新加的. all nan\n",
    "    normalized_test_data = (data_test - mean) / std  # 标准化\n",
    "    # print (\"normalized_test_data: \", normalized_test_data) # Brad新加的, all nan\n",
    "    size = (len(normalized_test_data) + time_step -\n",
    "            1) // time_step  # 有size个sample\n",
    "    test_x, test_y = [], []\n",
    "    for i in range(size - 1):\n",
    "        x = normalized_test_data[i * time_step:(i + 1) * time_step, :7]\n",
    "        y = normalized_test_data[i * time_step:(i + 1) * time_step, 7]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.extend(y)\n",
    "    test_x.append((normalized_test_data[(i + 1) * time_step:, :7]).tolist())\n",
    "    test_y.extend((normalized_test_data[(i + 1) * time_step:, 7]).tolist())\n",
    "    # print (\"test_x: \", test_x) # Brad新加，都是nan\n",
    "    # print (\"test_y\", test_y) # Brad新加，都是nan\n",
    "    return mean, std, test_x, test_y\n",
    "\n",
    "\n",
    "#——————————————————定义神经网络变量——————————————————\n",
    "# 输入层、输出层权重、偏置\n",
    "\n",
    "weights = {\n",
    "    'in': tf.Variable(tf.random_normal([input_size, rnn_unit])),\n",
    "    'out': tf.Variable(tf.random_normal([rnn_unit, 1]))\n",
    "}\n",
    "biases = {\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[rnn_unit, ])),\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[1, ]))\n",
    "}\n",
    "\n",
    "#——————————————————定义神经网络变量——————————————————\n",
    "\n",
    "\n",
    "def lstm(X):\n",
    "\n",
    "    batch_size = tf.shape(X)[0]\n",
    "    time_step = tf.shape(X)[1]\n",
    "    w_in = weights['in']\n",
    "    b_in = biases['in']\n",
    "    input = tf.reshape(X, [-1, input_size])  # 需要将tensor转成2维进行计算，计算后的结果作为隐藏层的输入\n",
    "    input_rnn = tf.matmul(input, w_in) + b_in\n",
    "    # 将tensor转成3维，作为lstm cell的输入\n",
    "    input_rnn = tf.reshape(input_rnn, [-1, time_step, rnn_unit])\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_unit)\n",
    "    init_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    output_rnn, final_states = tf.nn.dynamic_rnn(\n",
    "        cell, input_rnn, initial_state=init_state, dtype=tf.float32)\n",
    "    output = tf.reshape(output_rnn, [-1, rnn_unit])\n",
    "    w_out = weights['out']\n",
    "    b_out = biases['out']\n",
    "    pred = tf.matmul(output, w_out) + b_out\n",
    "    return pred, final_states\n",
    "\n",
    "#————————————————训练模型————————————————————\n",
    "\n",
    "# train_begin 从2000改成了100\n",
    "def train_lstm(batch_size=60, time_step=20, train_begin=0, train_end=1100):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, time_step, input_size])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, time_step, output_size])\n",
    "    batch_index, train_x, train_y = get_train_data(\n",
    "        batch_size, time_step, train_begin, train_end)\n",
    "    with tf.variable_scope(\"sec_lstm\"):\n",
    "        pred, _ = lstm(X)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.square(tf.reshape(pred, [-1]) - tf.reshape(Y, [-1])))\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=15)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(range_):  # 这个迭代次数，可以更改，越大预测效果会更好，但需要更长时间\n",
    "            for step in range(len(batch_index) - 1):\n",
    "                _, loss_ = sess.run([train_op, loss], feed_dict={X: train_x[batch_index[\n",
    "                                    step]:batch_index[step + 1]], Y: train_y[batch_index[step]:batch_index[step + 1]]})\n",
    "            print(\"Number of iterations:\", i, \" loss:\", loss_)\n",
    "        print(\"model_save5: \", saver.save(sess, 'model_save5\\\\modle.ckpt'))\n",
    "        # 我是在window下跑的，这个地址是存放模型的地方，模型参数文件名为modle.ckpt\n",
    "        # 在Linux下面用 'model_save2/modle.ckpt'\n",
    "        print(\"The train has finished\")\n",
    "train_lstm()\n",
    "\n",
    "#————————————————预测模型————————————————————\n",
    "\n",
    "\n",
    "def prediction(time_step=20):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, time_step, input_size])\n",
    "    mean, std, test_x, test_y = get_test_data(time_step)\n",
    "    with tf.variable_scope(\"sec_lstm\", reuse=True):\n",
    "        pred, _ = lstm(X)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        # 参数恢复\n",
    "        module_file = tf.train.latest_checkpoint('model_save5')\n",
    "        saver.restore(sess, module_file)\n",
    "        test_predict = []\n",
    "        # print (\"test_x\",test_x) # Brad新加的,也全是nan\n",
    "        for step in range(len(test_x) - 1):\n",
    "            prob = sess.run(pred, feed_dict={X: [test_x[step]]})\n",
    "            predict = prob.reshape((-1))\n",
    "            test_predict.extend(predict)\n",
    "        test_y = np.array(test_y) * std[7] + mean[7]\n",
    "        test_predict = np.array(test_predict) * std[7] + mean[7]\n",
    "        acc = np.average(np.abs(\n",
    "            test_predict - test_y[:len(test_predict)]) / test_y[:len(test_predict)])  # 偏差程度\n",
    "        # print (\"test_predict: \", test_predict)# Brad新加的, nan\n",
    "        # print (\"test_y: \", test_y)# Brad新加的,nan\n",
    "#         print(\"The accuracy of this predict:\", acc)\n",
    "        # 以折线图表示结果\n",
    "#         plt.figure()\n",
    "#         plt.plot(list(range(len(test_predict))), test_predict, color='b',)\n",
    "#         plt.plot(list(range(len(test_y))), test_y,  color='r')\n",
    "#         plt.show()\n",
    "        \n",
    "prediction()\n",
    "\n",
    "# def execute(rerun_times):\n",
    "#     for i in range(rerun_times):\n",
    "#         print (i)\n",
    "#         if i <= rerun_times:\n",
    "#             train_lstm()\n",
    "#             prediction()\n",
    "#         else:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# execute(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and record prediction rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.78397108  0.80333841  0.79002065  0.8092925 ]\n",
      " [ 0.80925965  0.806143    0.81008078  0.8064826 ]\n",
      " [ 0.80680104  0.80158555  0.79846702  0.79418927]\n",
      " ...\n",
      " [-0.20122939 -0.20315827 -0.19750071 -0.20086774]\n",
      " [-0.20052693 -0.20280769 -0.19714878 -0.19911155]\n",
      " [-0.19877078 -0.20280769 -0.19750071 -0.20086774]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-e9bb280e046a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m \u001b[0mtrain_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-e9bb280e046a>\u001b[0m in \u001b[0;36mtrain_lstm\u001b[1;34m(batch_size, time_step, train_begin, train_end)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     batch_index, train_x, train_y = get_train_data(batch_size, time_step,\n\u001b[1;32m--> 134\u001b[1;33m                                                    train_begin, train_end)\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sec_lstm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-e9bb280e046a>\u001b[0m in \u001b[0;36mget_train_data\u001b[1;34m(batch_size, time_step, train_begin, train_end)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalized_train_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# y.shape: (20, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalized_train_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "rnn_unit = 10  # 隐藏层单元数量\n",
    "input_size = 4\n",
    "output_size = 1\n",
    "lr = 0.0005  # 学习率\n",
    "location  = \"C:/Users/workspace/brad_public_workspace_on_win/SH_tongliang/data/数据库/火币日线/\"\n",
    "file = \"IF_cleaned_data_complete_version.csv\"\n",
    "# ——————————————————导入数据——————————————————————\n",
    "df = pd.read_csv(location+file, engine=\"python\")  # 读入IF期货数据\n",
    "data = df.iloc[:, [1, 2, 3, 4]].values  # numpy.ndarray, 几个特征量：开盘价、最高价、最低价、收盘价\n",
    "label = []\n",
    "for i in range(1, len(data)):\n",
    "    label.append(round(data[i, 3] - data[i - 1, 3], 4))\n",
    "\n",
    "df.loc[1:, 'label'] = label  # 标签，以收盘价差作为涨跌,含幅度变化\n",
    "date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# 获取训练集\n",
    "def get_train_data(batch_size=80,\n",
    "                   time_step=20,\n",
    "                   train_begin=0,\n",
    "                   train_end=120000):\n",
    "    batch_index = []\n",
    "    data_train = data[train_begin:train_end]\n",
    "    normalized_train_data = (\n",
    "        data_train - np.mean(data_train, axis=0)) / np.std(\n",
    "            data_train, axis=0)  # 标准化\n",
    "    # print('type(normalized_train_data): ', type(normalized_train_data))\n",
    "    # print('normalized_train_data.shape: ', normalized_train_data.shape)\n",
    "    # normalized_train_data.shape: (5800, 8)\n",
    "    train_x, train_y = [], []  # 训练集\n",
    "    print (normalized_train_data)\n",
    "    for i in range(len(normalized_train_data) - time_step):\n",
    "        if i % batch_size == 0:\n",
    "            batch_index.append(i)\n",
    "        # x.shape: (20, 7)\n",
    "        x = normalized_train_data[i:i + time_step, :3]\n",
    "        # y.shape: (20, 1)\n",
    "        y = normalized_train_data[i:i + time_step, 4, np.newaxis] \n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    # print('train_data x.shape: ', x.shape)\n",
    "    # train_data x.shape:  (20, 6)\n",
    "    # print('train_data y.shape: ', y.shape)\n",
    "    # train_data y.shape:  (20, 1)\n",
    "    # print('train_data y: ', y)\n",
    "    batch_index.append((len(normalized_train_data) - time_step))\n",
    "    return batch_index, train_x, train_y\n",
    "\n",
    "\n",
    "# 获取测试集\n",
    "def get_test_data(time_step=20, test_begin=120000):\n",
    "    data_test = data[test_begin:]\n",
    "    # print('data_test: ', data_test)\n",
    "    test_y = data_test[:, 4]\n",
    "    # print('test_y4: ', test_y)\n",
    "    # data_train = data[:test_begin]\n",
    "    mean = np.mean(data_test, axis=0)  # 改用已知的训练数据，避免用到未来数据\n",
    "    std = np.std(data_test, axis=0)  # 同上\n",
    "    normalized_test_data = (data_test - mean) / std  # 标准化\n",
    "    size = (\n",
    "        len(normalized_test_data) + time_step - 1) // time_step  # 有size个sample\n",
    "    test_x, test_y = [], []\n",
    "    for i in range(size - 1):\n",
    "        x = normalized_test_data[i * time_step:(i + 1) * time_step, :3]\n",
    "        y = normalized_test_data[i * time_step:(i + 1) * time_step, 4]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.extend(y)\n",
    "    # print('type(x): ', type(x))\n",
    "    # print('test_data x.shape: ', x.shape)\n",
    "    # print('test_data y.shape: ', y.shape)\n",
    "    # print('test_y3: ', test_y)\n",
    "    test_x.append((normalized_test_data[(i + 1) * time_step:, :3]).tolist())\n",
    "    test_y.extend((normalized_test_data[(i + 1) * time_step:, 4]).tolist())\n",
    "    return mean, std, test_x, test_y\n",
    "\n",
    "\n",
    "# ——————————————————定义神经网络变量——————————————————\n",
    "# 输入层、输出层的权重、偏置\n",
    "\n",
    "weights = {\n",
    "    'in': tf.Variable(tf.random_normal([input_size, rnn_unit])),\n",
    "    'out': tf.Variable(tf.random_normal([rnn_unit, 1]))\n",
    "}\n",
    "biases = {\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[\n",
    "        rnn_unit,\n",
    "    ])),\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[\n",
    "        1,\n",
    "    ]))\n",
    "}\n",
    "\n",
    "\n",
    "# ——————————————————定义神经网络变量——————————————————\n",
    "def lstm(X):\n",
    "\n",
    "    batch_size = tf.shape(X)[0]\n",
    "    time_step = tf.shape(X)[1]\n",
    "    w_in = weights['in']\n",
    "    b_in = biases['in']\n",
    "    input = tf.reshape(X, [-1, input_size])  # 需要将tensor转成2维进行计算，计算后的结果作为隐藏层的输入\n",
    "    input_rnn = tf.matmul(input, w_in) + b_in\n",
    "    input_rnn = tf.reshape(\n",
    "        input_rnn, [-1, time_step, rnn_unit])  # 将tensor转成3维，作为lstm cell的输入\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_unit)\n",
    "    init_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    output_rnn, final_states = tf.nn.dynamic_rnn(\n",
    "        cell, input_rnn, initial_state=init_state, dtype=tf.float32)\n",
    "    output = tf.reshape(output_rnn, [-1, rnn_unit])\n",
    "    w_out = weights['out']\n",
    "    b_out = biases['out']\n",
    "    pred = tf.matmul(output, w_out) + b_out\n",
    "    return pred, final_states\n",
    "\n",
    "\n",
    "# ————————————————训练模型————————————————————\n",
    "\n",
    "\n",
    "def train_lstm(batch_size=80, time_step=20, train_begin=0, train_end=120000):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, time_step, input_size])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, time_step, output_size])\n",
    "    batch_index, train_x, train_y = get_train_data(batch_size, time_step,\n",
    "                                                   train_begin, train_end)\n",
    "    with tf.variable_scope(\"sec_lstm\"):\n",
    "        pred, _ = lstm(X)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.square(tf.reshape(pred, [-1]) - tf.reshape(Y, [-1])))\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=15)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(200):  # 这个迭代次数，可以更改，越大预测效果会更好，但需要更长时间\n",
    "            for step in range(len(batch_index) - 1):\n",
    "                _, loss_ = sess.run(\n",
    "                    [train_op, loss],\n",
    "                    feed_dict={\n",
    "                        X: train_x[batch_index[step]:batch_index[step + 1]],\n",
    "                        Y: train_y[batch_index[step]:batch_index[step + 1]]\n",
    "                    })\n",
    "            print(\"Number of iterations:\", i, \" loss:\", loss_)\n",
    "            with open( location+ 'loss_' + date + '.csv', 'w') as f:\n",
    "                f.write(str(loss_) + '\\n')\n",
    "        # print(\"model_save: \", saver.save(sess, 'model_save2\\\\modle.ckpt'))\n",
    "        print(\"model_save: \", saver.save(sess, location+'model_save5/modle.ckpt'))\n",
    "        # 我是在window下跑的，这个地址是存放模型的地方，模型参数文件名为modle.ckpt\n",
    "        # 在Linux下面用 'model_save2/modle.ckpt'\n",
    "        print(\"The train has finished\")\n",
    "\n",
    "\n",
    "train_lstm()\n",
    "\n",
    "\n",
    "# ————————————————预测模型————————————————————\n",
    "def prediction(time_step=20):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, time_step, input_size])\n",
    "    mean, std, test_x, test_y = get_test_data(time_step)\n",
    "    with tf.variable_scope(\"sec_lstm\", reuse=True):\n",
    "        pred, _ = lstm(X)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        # 参数恢复\n",
    "        module_file = tf.train.latest_checkpoint('model_save5')\n",
    "        saver.restore(sess, module_file)\n",
    "        test_predict = []\n",
    "        for step in range(len(test_x) - 1):\n",
    "            # prob.shape: (20, 1)\n",
    "            prob = sess.run(pred, feed_dict={X: [test_x[step]]})\n",
    "            # print('prob.shape: ', prob.shape)\n",
    "            # predict.shape: (20, )\n",
    "            predict = prob.reshape((-1))\n",
    "            test_predict.extend(predict)\n",
    "        # print('test_y1: ', test_y)\n",
    "        test_y = np.array(test_y) * std[4] + mean[4]\n",
    "        # print('test_y2: ', test_y)\n",
    "        # print('test_y.shape: ', test_y.shape)\n",
    "        test_predict = np.array(test_predict) * std[4] + mean[4]\n",
    "        acc = np.average(\n",
    "            np.abs(test_predict - test_y[:len(test_predict)]) /\n",
    "            test_y[:len(test_predict)])  # 偏差程度\n",
    "        print(\"The accuracy of this predicet:\", 1 - acc)\n",
    "        with open(location+'test_y_' + date + '.csv', 'w') as f:\n",
    "            f.write(str(test_y) + '\\n')\n",
    "\n",
    "        with open(location+'test_predict_' + date + '.csv', 'w') as f:\n",
    "            f.write(str(test_predict))\n",
    "\n",
    "        with open(location+'accuracy_' + date + '.csv', 'w') as f:\n",
    "            f.write(str(1-acc) + '\\n')\n",
    "        # 以折线图表示结果\n",
    "        # print('len(test_predict): ', len(test_predict))\n",
    "        # print('len(test_y): ', len(test_y))\n",
    "        # print('len(data): ', len(data))\n",
    "        plt.figure()\n",
    "        # fig = plt.subplot(111)\n",
    "        plt.plot(\n",
    "            list(range(len(test_predict))),\n",
    "            test_predict,\n",
    "            color='blue',\n",
    "            label='predict',\n",
    "        )\n",
    "        plt.plot(list(range(len(test_y))), test_y, color='red', label='real')\n",
    "        # plt.plot(\n",
    "        #     list(range(len(data))), data, color='r', label='real_all')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.ylabel('high')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "prediction()\n",
    "# The accuracy of this predict: 0.016965209201494337\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = normalized_train_data[i:i + time_step, 4, np.newaxis] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8d6d3b613bfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# x2 = np.array([5, 4, 3])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "x1 = np.array([1, 2, 3, 4], [2, 3, 4, 4])\n",
    "# x2 = np.array([5, 4, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 3, 2, 6], [2, 3, 5, 1], \n",
    "          [3, 2, 5, 1], [2, 4, 1, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 2, 6],\n",
       "       [2, 3, 5, 1],\n",
       "       [3, 2, 5, 1],\n",
       "       [2, 4, 1, 6]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        x = normalized_train_data[i:i + time_step, :3]\n",
    "        # y.shape: (20, 1)\n",
    "        y = normalized_train_data[i:i + time_step, 4, np.newaxis] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 2],\n",
       "       [2, 3, 5],\n",
       "       [3, 2, 5]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3, 3, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正在进行的米筐机器学习算法代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'structure'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-076516269381>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpybrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequentialDataSet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpybrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshortcuts\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuildNetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpybrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pybrain\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'structure'"
     ]
    }
   ],
   "source": [
    "# 可以自己import我们平台支持的第三方python模块，比如pandas、numpy等。\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from pybrain.datasets import SequentialDataSet\n",
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.structure.networks import Network\n",
    "from pybrain.structure.modules import ReluLayer, LSTMLayer\n",
    "from pybrain.supervised import RPropMinusTrainer\n",
    "\n",
    "# 训练trainX和trainY，并返回神经网络net\n",
    "def train(context, trainX, trainY):\n",
    "    ds = SequentialDataSet(4, 1)\n",
    "    for dataX, dataY in zip(trainX, trainY):\n",
    "        ds.addSample(dataX, dataY)\n",
    "    net = buildNetwork(4, 1, 1, hiddenclass=LSTMLayer, outputbias=False, recurrent=True)\n",
    "    trainer = RPropMinusTrainer(net, dataset=ds)\n",
    "    EPOCHS_PER_CYCLE = 5\n",
    "    CYCLES = 5\n",
    "    for i in range(CYCLES):\n",
    "        trainer.trainEpochs(EPOCHS_PER_CYCLE)\n",
    "    return net, trainer.testOnData()\n",
    "\n",
    "# 更新数据集data\n",
    "def load(context, ticker):\n",
    "    close = history(90, '1d', 'close')[ticker]\n",
    "    high  = history(90, '1d', 'high')[ticker]\n",
    "    low   = history(90, '1d', 'low')[ticker]\n",
    "    volume= history(90, '1d', 'volume')[ticker]\n",
    "    data = pd.DataFrame({'close':close.values,\n",
    "                         'high':high.values,\n",
    "                         'low':low.values,\n",
    "                         'volume':volume.values},index=close.index)\n",
    "    # print (data)\n",
    "    context.position_ratio.append([data['close'].mean(),\n",
    "                                   data['high'].mean(), \n",
    "                                   data['low'].mean(), \n",
    "                                   data['volume'].mean()])\n",
    "    context.shape_ratio.append([data['close'].std(),\n",
    "                                data['high'].std(),\n",
    "                                data['low'].std(),\n",
    "                                data['volume'].std()])\n",
    "    data['close']  = (data['close'] - context.position_ratio[-1][0]) / context.shape_ratio[-1][0]\n",
    "    data['high']   = (data['high'] - context.position_ratio[-1][1]) / context.shape_ratio[-1][1]\n",
    "    data['low']    = (data['low'] - context.position_ratio[-1][2]) / context.shape_ratio[-1][2]\n",
    "    data['volume'] = (data['volume'] - context.position_ratio[-1][3]) / context.shape_ratio[-1][3]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 剔除情况特殊的黑名单股，只看策略效果，排除个体问题\n",
    "def filter_blacklist(context, stock_list):\n",
    "    return [ticker for ticker in stock_list if ticker not in context.blacklist]\n",
    "    \n",
    "def filter_stlist(stock_list):\n",
    "    return [ticker for ticker in stock_list if not is_st_stock(ticker)]\n",
    "\n",
    "# 建模，每3个月运行一次，用过去6个月训练\n",
    "def modelize(context, bar_dict):\n",
    "    if context.every_3_months % 3 != 0: \n",
    "        context.every_3_months += 1\n",
    "        return 0\n",
    "    print('-'*65)\n",
    "    print('------'+'{:-^59}'.format('modelizing'))\n",
    "    context.position_ratio = []\n",
    "    context.shape_ratio    = []\n",
    "    context.data  = []\n",
    "    context.net   = []\n",
    "    context.list  = []\n",
    "    templist = list(get_fundamentals(query(fundamentals.eod_derivative_indicator.market_cap)\n",
    "                                    .order_by(fundamentals.eod_derivative_indicator.market_cap.asc())\n",
    "                                    .limit(context.num*5)).columns)\n",
    "    context.list = filter_blacklist(context, filter_stlist(templist))[:context.num]\n",
    "    names  = []\n",
    "    scores = []\n",
    "    for ticker in context.list:\n",
    "        names.append('{:<11}'.format(ticker))\n",
    "        data = load(context, ticker)\n",
    "        trainX = data.ix[:-1,:].values\n",
    "        trainY = data.ix[1:,0].values\n",
    "        net, mse = train(context, trainX, trainY)\n",
    "        context.data.append(data)\n",
    "        context.net.append(net)\n",
    "        scores.append('{:<11}'.format(str(mse)[:6]))\n",
    "        if np.isnan(mse):\n",
    "            context.blacklist.append(ticker)\n",
    "            context.mflag = 0\n",
    "            return 0\n",
    "    context.pct = [0] * context.num\n",
    "    print('------'+'{:-^59}'.format('finished'))\n",
    "    print('-'*65)\n",
    "    print(' nm | '+' '.join(names))\n",
    "    print('mse | '+' '.join(scores))\n",
    "\n",
    "    context.mflag = 1  # 标记已经建模\n",
    "    context.tflag = 0\n",
    "    context.every_3_months += 1\n",
    "\n",
    "def mkt_panic():\n",
    "    # 连续两天大盘跌破3个点，或者大盘跌破5个点\n",
    "    mkt = history(3, '1d', 'close')['000001.XSHG']\n",
    "    panic = (mkt[-1]/mkt[-2] < 0.97 and mkt[-2]/mkt[-3] < 0.97) or mkt[-1]/mkt[-2] < 0.95\n",
    "    if panic:\n",
    "        print('!!!!!!'+'{:!^59}'.format('panic'))\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# 最后利用每3个月更新的模型，每天进行交易，预测涨幅超过a就买入，预测跌幅超过b则卖出\n",
    "def trade(context,bar_dict):\n",
    "    \n",
    "    while context.mflag == 0: modelize(context, bar_dict)\n",
    "    \n",
    "    trash_bin = [ticker for ticker in context.portfolio.positions if ticker not in context.list]\n",
    "    for ticker in trash_bin: order_target_percent(ticker, 0)\n",
    "    \n",
    "    actual_close  = []\n",
    "    actual_high   = []\n",
    "    actual_low    = []\n",
    "    actual_vol    = []\n",
    "    actual_open   = []\n",
    "    actual_data   = []\n",
    "    predict_close = []\n",
    "    \n",
    "    for i in range(context.num):\n",
    "        actual_close.append((history(1,'1d','close')[context.list[i]][0] - context.position_ratio[i][0]) / context.shape_ratio[i][0])\n",
    "        actual_high.append((history(1,'1d','high')[context.list[i]][0] - context.position_ratio[i][1]) / context.shape_ratio[i][1])\n",
    "        actual_low.append((history(1,'1d','low')[context.list[i]][0] - context.position_ratio[i][2]) / context.shape_ratio[i][2])\n",
    "        actual_vol.append((history(1,'1d','volume')[context.list[i]][0] - context.position_ratio[i][3]) / context.shape_ratio[i][3])\n",
    "        actual_open.append((history(1,'1m','close')[context.list[i]][0] - context.position_ratio[i][0]) / context.shape_ratio[i][0])\n",
    "        actual_data.append([actual_close[i],actual_high[i],actual_low[i],actual_vol[i]])\n",
    "        predict_close.append(context.net[i].activate(actual_data[i])[0])\n",
    "\n",
    "    if context.tflag == 0: \n",
    "        context.temp_pc = predict_close\n",
    "    \n",
    "    r = [float((pc*shape_ratio[0]+position_ratio[0]) / (ao*shape_ratio[0]+position_ratio[0]) - 1) for pc, ao, shape_ratio, position_ratio in zip(predict_close, actual_open, context.shape_ratio, context.position_ratio)]\n",
    "    \n",
    "    temp_r = [float((pc*shape_ratio[0]+position_ratio[0]) / (tpc*shape_ratio[0]+position_ratio[0]) - 1) for pc, tpc, shape_ratio, position_ratio in zip(predict_close, context.temp_pc, context.shape_ratio, context.position_ratio)]\n",
    "    \n",
    "    # The essence of this strategy\n",
    "    hybrid_r = [max(ri,temp_ri,ri+temp_ri) for ri, temp_ri in zip(r,temp_r)]\n",
    "    bad_hybrid_signal = sum([x <= 0 for x in hybrid_r])\n",
    "    a, b = 0.00, -0.01\n",
    "    panic = mkt_panic()\n",
    "    for i in range(context.num):\n",
    "        if panic or 0 < context.post_panic < 22 * context.num:\n",
    "            context.pct[i] = 0\n",
    "            context.post_panic = (1 - panic) * (context.post_panic + 1) + panic\n",
    "        elif hybrid_r[i] > a:\n",
    "            context.pct[i] = min(context.pct[i] + .5/context.num, 2/context.num)\n",
    "            context.post_panic = 0\n",
    "        elif hybrid_r[i] < b or bad_hybrid_signal > 3*context.num//5:\n",
    "            context.pct[i] = max(context.pct[i] - .5/context.num, 0)\n",
    "            context.post_panic = 0\n",
    "        \n",
    "\n",
    "    if context.tflag == 1: print(' ac | '+' '.join(['{:<11}'.format(str(ac)[:6]) for ac in actual_close]))\n",
    "    print('-'*65)\n",
    "    print(' ao | '+' '.join(['{:<11}'.format(str(ao)[:6]) for ao in actual_open]))\n",
    "    print(' pc | '+' '.join(['{:<11}'.format(str(pc)[:6]) for pc in predict_close]))\n",
    "    print('  r | '+' '.join(['{:<11}'.format(str(ri)[:6]) for ri in hybrid_r]))\n",
    "    pct = sum([context.portfolio.positions[ticker].market_value for ticker in context.portfolio.positions])/(context.portfolio.market_value+context.portfolio.cash)\n",
    "    tot_pct = max(sum(context.pct), 1)\n",
    "    context.pct = list(map(lambda x: x/tot_pct, context.pct))\n",
    "    print('  % | '+' '.join(['{:<11}'.format(str(p)[:6]) for p in context.pct]))\n",
    "    plot('total position', pct * 100)\n",
    "    for i in range(context.num): order_target_percent(context.list[i], context.pct[i])\n",
    "    context.tflag = 1\n",
    "    context.temp_pc = predict_close\n",
    "    \n",
    "# 在这个方法中编写任何的初始化逻辑。context对象将会在你的算法策略的任何方法之间做传递。\n",
    "def init(context):\n",
    "    context.temp_pc        = []\n",
    "    context.every_3_months = 0\n",
    "    context.tflag          = 0\n",
    "    context.mflag          = 0\n",
    "    context.position_ratio = []\n",
    "    context.shape_ratio    = []\n",
    "    context.num            = 20\n",
    "    context.list           = []\n",
    "    context.pct            = [0] * context.num\n",
    "    context.net            = []\n",
    "    context.data           = []\n",
    "    context.post_panic     = 0\n",
    "    context.blacklist      = [\n",
    "                              '000004.XSHE','000546.XSHE',\n",
    "                              '000594.XSHE','002352.XSHE',\n",
    "                              '300176.XSHE','300260.XSHE',\n",
    "                              '300372.XSHE','600137.XSHG',\n",
    "                              '600306.XSHG','600656.XSHG',\n",
    "                             ]\n",
    "    scheduler.run_monthly(modelize,1)\n",
    "    scheduler.run_daily(trade, time_rule=market_open(minute=1))\n",
    "\n",
    "# before_trading此函数会在每天交易开始前被调用，当天只会被调用一次\n",
    "def before_trading(context):\n",
    "    pass\n",
    "\n",
    "# 你选择的证券的数据更新将会触发此段逻辑，例如日或分钟历史数据切片或者是实时数据切片更新\n",
    "def handle_bar(context, bar_dict):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
