{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_unit = 10  # 隐藏层单元数量\n",
    "input_size = 4\n",
    "output_size = 1\n",
    "lr = 0.0005  # 学习率\n",
    "location  = \"C:/Users/workspace/brad_public_workspace_on_win/SH_tongliang/data/数据库/火币日线/\"\n",
    "file = \"11_14_huobi_btcusdt_copy_for_ML.csv\"\n",
    "# ——————————————————导入数据——————————————————————\n",
    "df = pd.read_csv(location+file, engine=\"python\")  # 读入IF期货数据\n",
    "df.loc[1:, 'label'] = label  # 标签，以收盘价差作为涨跌,含幅度变化\n",
    "data = df.iloc[:, [1, 2, 3, 4, 5]].values  # numpy.ndarray, 几个特征量：开盘价、最高价、最低价、收盘价\n",
    "label = []\n",
    "for i in range(1, len(data)):\n",
    "    label.append(round(data[i, 3] - data[i - 1, 3], 4))\n",
    "\n",
    "date = datetime.now().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数一：获取训练集\n",
    "def get_train_data(batch_size=80,\n",
    "                   time_step=20,\n",
    "                   train_begin=0,\n",
    "                   train_end=120000):\n",
    "    pass\n",
    "\n",
    "batch_size=80\n",
    "time_step=20\n",
    "train_begin=0\n",
    "train_end=1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_index = []\n",
    "data_train = data[train_begin:train_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.03317008 -0.96123828 -1.13259015 -0.97458042         nan]\n",
      " [-0.97319774 -0.97516928 -1.02307114 -1.02468269         nan]\n",
      " [-1.0216766  -1.02780351 -1.03617625 -1.05409779         nan]\n",
      " ...\n",
      " [-0.76160875 -0.79996705 -0.7438997  -0.78993281         nan]\n",
      " [-0.78868407 -0.7948252  -0.74069037 -0.76421381         nan]\n",
      " [-0.76270215 -0.80003086 -0.72947596 -0.76084707         nan]]\n"
     ]
    }
   ],
   "source": [
    "normalized_train_data = (\n",
    "    data_train - np.mean(data_train, axis=0)) / np.std(\n",
    "        data_train, axis=0)  # 标准化\n",
    "# print('type(normalized_train_data): ', type(normalized_train_data))\n",
    "# print('normalized_train_data.shape: ', normalized_train_data.shape)\n",
    "# normalized_train_data.shape: (5800, 8)\n",
    "train_x, train_y = [], []  # 训练集\n",
    "print (normalized_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.03317008, -0.96123828, -1.13259015, -0.97458042,         nan],\n",
       "       [-0.97319774, -0.97516928, -1.02307114, -1.02468269,         nan],\n",
       "       [-1.0216766 , -1.02780351, -1.03617625, -1.05409779,         nan],\n",
       "       ...,\n",
       "       [-0.76160875, -0.79996705, -0.7438997 , -0.78993281,         nan],\n",
       "       [-0.78868407, -0.7948252 , -0.74069037, -0.76421381,         nan],\n",
       "       [-0.76270215, -0.80003086, -0.72947596, -0.76084707,         nan]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.97458042],\n",
       "       [-1.02468269],\n",
       "       [-1.05409779],\n",
       "       [-0.9522428 ],\n",
       "       [-0.90102804],\n",
       "       [-0.80487456],\n",
       "       [-0.72323834],\n",
       "       [-0.55364959],\n",
       "       [-0.44700797],\n",
       "       [-0.4515421 ],\n",
       "       [-0.37404477],\n",
       "       [-0.47896642],\n",
       "       [-0.5602184 ],\n",
       "       [-0.39059305],\n",
       "       [-0.51124325],\n",
       "       [-0.63139942],\n",
       "       [-0.88493354],\n",
       "       [-0.8770839 ],\n",
       "       [-0.69324504],\n",
       "       [-0.76232551]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_train_data[0:20, 3, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_train_data[0:20, 4, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.03317008, -0.96123828, -1.13259015],\n",
       "       [-0.97319774, -0.97516928, -1.02307114],\n",
       "       [-1.0216766 , -1.02780351, -1.03617625],\n",
       "       [-1.05348022, -0.96550355, -1.03195665],\n",
       "       [-0.94951232, -0.88540362, -0.93986785],\n",
       "       [-0.8997426 , -0.82860823, -0.8683131 ],\n",
       "       [-0.80381977, -0.75948719, -0.77351808],\n",
       "       [-0.72248412, -0.58844612, -0.66186898],\n",
       "       [-0.55260516, -0.51358543, -0.54427592],\n",
       "       [-0.44628956, -0.51507324, -0.49664308],\n",
       "       [-0.45057173, -0.4399808 , -0.35960333],\n",
       "       [-0.37462262, -0.44138129, -0.42255658],\n",
       "       [-0.47797617, -0.53975476, -0.49574642],\n",
       "       [-0.55914726, -0.45860362, -0.49598986],\n",
       "       [-0.39081881, -0.39186711, -0.45795256],\n",
       "       [-0.51022592, -0.52541738, -0.60249023],\n",
       "       [-0.63029859, -0.64191996, -0.87266254],\n",
       "       [-0.8839706 , -0.78995539, -1.21170772],\n",
       "       [-0.87662034, -0.71260606, -1.03156715],\n",
       "       [-0.6920445 , -0.73916498, -0.79599558]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_train_data[i:i + time_step, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-e16ca717a098>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnormalized_train_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "normalized_train_data[i:i + time_step, 4, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(normalized_train_data) - time_step):\n",
    "    if i % batch_size == 0:\n",
    "        batch_index.append(i)\n",
    "    # x.shape: (20, 7)\n",
    "    x = normalized_train_data[i:i + time_step, :3]\n",
    "    # y.shape: (20, 1)\n",
    "    y = normalized_train_data[i:i + time_step, 3, np.newaxis] \n",
    "    train_x.append(x.tolist())\n",
    "    train_y.append(y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.03317008 -0.96123828 -1.13259015 -0.97458042]\n",
      " [-0.97319774 -0.97516928 -1.02307114 -1.02468269]\n",
      " [-1.0216766  -1.02780351 -1.03617625 -1.05409779]\n",
      " ...\n",
      " [-0.76160875 -0.79996705 -0.7438997  -0.78993281]\n",
      " [-0.78868407 -0.7948252  -0.74069037 -0.76421381]\n",
      " [-0.76270215 -0.80003086 -0.72947596 -0.76084707]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b5b5a837bef4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m \u001b[0mtrain_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-b5b5a837bef4>\u001b[0m in \u001b[0;36mtrain_lstm\u001b[1;34m(batch_size, time_step, train_begin, train_end)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     batch_index, train_x, train_y = get_train_data(batch_size, time_step,\n\u001b[1;32m--> 107\u001b[1;33m                                                    train_begin, train_end)\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sec_lstm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-b5b5a837bef4>\u001b[0m in \u001b[0;36mget_train_data\u001b[1;34m(batch_size, time_step, train_begin, train_end)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalized_train_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# y.shape: (20, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalized_train_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "\n",
    "# print('train_data x.shape: ', x.shape)\n",
    "# train_data x.shape:  (20, 6)\n",
    "# print('train_data y.shape: ', y.shape)\n",
    "# train_data y.shape:  (20, 1)\n",
    "# print('train_data y: ', y)\n",
    "batch_index.append((len(normalized_train_data) - time_step))\n",
    "#     return batch_index, train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.03317008 -0.96123828 -1.13259015 -0.97458042         nan]\n",
      " [-0.97319774 -0.97516928 -1.02307114 -1.02468269         nan]\n",
      " [-1.0216766  -1.02780351 -1.03617625 -1.05409779         nan]\n",
      " ...\n",
      " [-0.76160875 -0.79996705 -0.7438997  -0.78993281         nan]\n",
      " [-0.78868407 -0.7948252  -0.74069037 -0.76421381         nan]\n",
      " [-0.76270215 -0.80003086 -0.72947596 -0.76084707         nan]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (80, 20, 3) for Tensor 'Placeholder_2:0', which has shape '(?, 20, 4)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-b5b5a837bef4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m \u001b[0mtrain_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-68-b5b5a837bef4>\u001b[0m in \u001b[0;36mtrain_lstm\u001b[1;34m(batch_size, time_step, train_begin, train_end)\u001b[0m\n\u001b[0;32m    121\u001b[0m                     feed_dict={\n\u001b[0;32m    122\u001b[0m                         \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                         \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m                     })\n\u001b[0;32m    125\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of iterations:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" loss:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1109\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1111\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1112\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (80, 20, 3) for Tensor 'Placeholder_2:0', which has shape '(?, 20, 4)'"
     ]
    }
   ],
   "source": [
    "# 获取训练集\n",
    "def get_train_data(batch_size=80,\n",
    "                   time_step=20,\n",
    "                   train_begin=0,\n",
    "                   train_end=120000):\n",
    "    batch_index = []\n",
    "    data_train = data[train_begin:train_end]\n",
    "    normalized_train_data = (\n",
    "        data_train - np.mean(data_train, axis=0)) / np.std(\n",
    "            data_train, axis=0)  # 标准化\n",
    "    # print('type(normalized_train_data): ', type(normalized_train_data))\n",
    "    # print('normalized_train_data.shape: ', normalized_train_data.shape)\n",
    "    # normalized_train_data.shape: (5800, 8)\n",
    "    train_x, train_y = [], []  # 训练集\n",
    "    print (normalized_train_data)\n",
    "    for i in range(len(normalized_train_data) - time_step):\n",
    "        if i % batch_size == 0:\n",
    "            batch_index.append(i)\n",
    "        # x.shape: (20, 7)\n",
    "        x = normalized_train_data[i:i + time_step, :3]\n",
    "        # y.shape: (20, 1)\n",
    "        y = normalized_train_data[i:i + time_step, 4, np.newaxis] \n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    # print('train_data x.shape: ', x.shape)\n",
    "    # train_data x.shape:  (20, 6)\n",
    "    # print('train_data y.shape: ', y.shape)\n",
    "    # train_data y.shape:  (20, 1)\n",
    "    # print('train_data y: ', y)\n",
    "    batch_index.append((len(normalized_train_data) - time_step))\n",
    "    return batch_index, train_x, train_y\n",
    "\n",
    "\n",
    "# 获取测试集\n",
    "def get_test_data(time_step=20, test_begin=120000):\n",
    "    data_test = data[test_begin:]\n",
    "    # print('data_test: ', data_test)\n",
    "    test_y = data_test[:, 4]\n",
    "    # print('test_y4: ', test_y)\n",
    "    # data_train = data[:test_begin]\n",
    "    mean = np.mean(data_test, axis=0)  # 改用已知的训练数据，避免用到未来数据\n",
    "    std = np.std(data_test, axis=0)  # 同上\n",
    "    normalized_test_data = (data_test - mean) / std  # 标准化\n",
    "    size = (\n",
    "        len(normalized_test_data) + time_step - 1) // time_step  # 有size个sample\n",
    "    test_x, test_y = [], []\n",
    "    for i in range(size - 1):\n",
    "        x = normalized_test_data[i * time_step:(i + 1) * time_step, :3]\n",
    "        y = normalized_test_data[i * time_step:(i + 1) * time_step, 4]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.extend(y)\n",
    "    # print('type(x): ', type(x))\n",
    "    # print('test_data x.shape: ', x.shape)\n",
    "    # print('test_data y.shape: ', y.shape)\n",
    "    # print('test_y3: ', test_y)\n",
    "    test_x.append((normalized_test_data[(i + 1) * time_step:, :3]).tolist())\n",
    "    test_y.extend((normalized_test_data[(i + 1) * time_step:, 4]).tolist())\n",
    "    return mean, std, test_x, test_y\n",
    "\n",
    "\n",
    "# ——————————————————定义神经网络变量——————————————————\n",
    "# 输入层、输出层的权重、偏置\n",
    "\n",
    "weights = {\n",
    "    'in': tf.Variable(tf.random_normal([input_size, rnn_unit])),\n",
    "    'out': tf.Variable(tf.random_normal([rnn_unit, 1]))\n",
    "}\n",
    "biases = {\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[\n",
    "        rnn_unit,\n",
    "    ])),\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[\n",
    "        1,\n",
    "    ]))\n",
    "}\n",
    "\n",
    "\n",
    "# ——————————————————定义神经网络变量——————————————————\n",
    "def lstm(X):\n",
    "\n",
    "    batch_size = tf.shape(X)[0]\n",
    "    time_step = tf.shape(X)[1]\n",
    "    w_in = weights['in']\n",
    "    b_in = biases['in']\n",
    "    input = tf.reshape(X, [-1, input_size])  # 需要将tensor转成2维进行计算，计算后的结果作为隐藏层的输入\n",
    "    input_rnn = tf.matmul(input, w_in) + b_in\n",
    "    input_rnn = tf.reshape(\n",
    "        input_rnn, [-1, time_step, rnn_unit])  # 将tensor转成3维，作为lstm cell的输入\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_unit)\n",
    "    init_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    output_rnn, final_states = tf.nn.dynamic_rnn(\n",
    "        cell, input_rnn, initial_state=init_state, dtype=tf.float32)\n",
    "    output = tf.reshape(output_rnn, [-1, rnn_unit])\n",
    "    w_out = weights['out']\n",
    "    b_out = biases['out']\n",
    "    pred = tf.matmul(output, w_out) + b_out\n",
    "    return pred, final_states\n",
    "\n",
    "\n",
    "# ————————————————训练模型————————————————————\n",
    "\n",
    "\n",
    "def train_lstm(batch_size=80, time_step=20, train_begin=0, train_end=120000):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, time_step, input_size])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, time_step, output_size])\n",
    "    batch_index, train_x, train_y = get_train_data(batch_size, time_step,\n",
    "                                                   train_begin, train_end)\n",
    "    with tf.variable_scope(\"sec_lstm\"):\n",
    "        pred, _ = lstm(X)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.square(tf.reshape(pred, [-1]) - tf.reshape(Y, [-1])))\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=15)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(200):  # 这个迭代次数，可以更改，越大预测效果会更好，但需要更长时间\n",
    "            for step in range(len(batch_index) - 1):\n",
    "                _, loss_ = sess.run(\n",
    "                    [train_op, loss],\n",
    "                    feed_dict={\n",
    "                        X: train_x[batch_index[step]:batch_index[step + 1]],\n",
    "                        Y: train_y[batch_index[step]:batch_index[step + 1]]\n",
    "                    })\n",
    "            print(\"Number of iterations:\", i, \" loss:\", loss_)\n",
    "            with open( location+ 'loss_' + date + '.csv', 'w') as f:\n",
    "                f.write(str(loss_) + '\\n')\n",
    "        # print(\"model_save: \", saver.save(sess, 'model_save2\\\\modle.ckpt'))\n",
    "        print(\"model_save: \", saver.save(sess, location+'model_save5/modle.ckpt'))\n",
    "        # 我是在window下跑的，这个地址是存放模型的地方，模型参数文件名为modle.ckpt\n",
    "        # 在Linux下面用 'model_save2/modle.ckpt'\n",
    "        print(\"The train has finished\")\n",
    "\n",
    "\n",
    "train_lstm()\n",
    "\n",
    "\n",
    "# ————————————————预测模型————————————————————\n",
    "def prediction(time_step=20):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, time_step, input_size])\n",
    "    mean, std, test_x, test_y = get_test_data(time_step)\n",
    "    with tf.variable_scope(\"sec_lstm\", reuse=True):\n",
    "        pred, _ = lstm(X)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        # 参数恢复\n",
    "        module_file = tf.train.latest_checkpoint('model_save5')\n",
    "        saver.restore(sess, module_file)\n",
    "        test_predict = []\n",
    "        for step in range(len(test_x) - 1):\n",
    "            # prob.shape: (20, 1)\n",
    "            prob = sess.run(pred, feed_dict={X: [test_x[step]]})\n",
    "            # print('prob.shape: ', prob.shape)\n",
    "            # predict.shape: (20, )\n",
    "            predict = prob.reshape((-1))\n",
    "            test_predict.extend(predict)\n",
    "        # print('test_y1: ', test_y)\n",
    "        test_y = np.array(test_y) * std[4] + mean[4]\n",
    "        # print('test_y2: ', test_y)\n",
    "        # print('test_y.shape: ', test_y.shape)\n",
    "        test_predict = np.array(test_predict) * std[4] + mean[4]\n",
    "        acc = np.average(\n",
    "            np.abs(test_predict - test_y[:len(test_predict)]) /\n",
    "            test_y[:len(test_predict)])  # 偏差程度\n",
    "        print(\"The accuracy of this predicet:\", 1 - acc)\n",
    "        with open(location+'test_y_' + date + '.csv', 'w') as f:\n",
    "            f.write(str(test_y) + '\\n')\n",
    "\n",
    "        with open(location+'test_predict_' + date + '.csv', 'w') as f:\n",
    "            f.write(str(test_predict))\n",
    "\n",
    "        with open(location+'accuracy_' + date + '.csv', 'w') as f:\n",
    "            f.write(str(1-acc) + '\\n')\n",
    "        # 以折线图表示结果\n",
    "        # print('len(test_predict): ', len(test_predict))\n",
    "        # print('len(test_y): ', len(test_y))\n",
    "        # print('len(data): ', len(data))\n",
    "        plt.figure()\n",
    "        # fig = plt.subplot(111)\n",
    "        plt.plot(\n",
    "            list(range(len(test_predict))),\n",
    "            test_predict,\n",
    "            color='blue',\n",
    "            label='predict',\n",
    "        )\n",
    "        plt.plot(list(range(len(test_y))), test_y, color='red', label='real')\n",
    "        # plt.plot(\n",
    "        #     list(range(len(data))), data, color='r', label='real_all')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.ylabel('high')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "prediction()\n",
    "# The accuracy of this predict: 0.016965209201494337"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
