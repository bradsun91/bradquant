{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import tensorflow as tf\n",
    "from logging_future import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"C:/Users/workspace/brad_public_workspace_on_win/non_code_files_brad_public_workspace_on_win/brad_public_workspace_on_win_non_code_files/SH_tongliang/data/11_22_crypto_data/\" \n",
    "\n",
    "file1 = \"res_11_21_adau18z18乘10000000.csv\"\n",
    "file2 = \"res_11_21_bchu18z18乘1000.csv\"\n",
    "file3 = \"res_11_21_xbtusd乘1.csv\"\n",
    "file4 = \"res_11_21_eosu18z18乘100000.csv\"\n",
    "file5 = \"res_11_21_ethu18z18乘10000.csv\"\n",
    "file6 = \"res_11_21_ltcu18z18乘10000.csv\"\n",
    "file7 = \"res_11_21_trxu18z18乘100000000.csv\"\n",
    "file8 = \"res_11_21_xrpu18z18乘10000000.csv\"\n",
    "\n",
    "df_ada = pd.read_csv(location + file1, engine=\"python\")\n",
    "df_bch = pd.read_csv(location + file2, engine=\"python\")\n",
    "df_btc = pd.read_csv(location + file3, engine=\"python\")\n",
    "df_eos = pd.read_csv(location + file4, engine=\"python\")\n",
    "df_eth = pd.read_csv(location + file5, engine=\"python\")\n",
    "df_ltc = pd.read_csv(location + file6, engine=\"python\")\n",
    "df_trx = pd.read_csv(location + file7, engine=\"python\")\n",
    "df_xrp = pd.read_csv(location + file8, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vars and paras lists:\n",
    "\n",
    "\"\"\"\n",
    "Here we will use for para in paras_to_optimize:\n",
    "    ...\n",
    "\"\"\"\n",
    "\n",
    "# df and df_asst_og are the same\n",
    "def lstm_all_in_one_optimization(df, df_asst_og, asst, test_train_split, time_step, rnn_unit, batch_size, train_times, lr):\n",
    "    # Use this code to prevent the kernel from requesting to start again everytime we run the model. \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # create func to record paras each time we finish running\n",
    "    optimization_record = {'test_train_split': [],\n",
    "                           'time_step': [],\n",
    "                           'rnn_unit': [],\n",
    "                           'batch_size': [],\n",
    "                           'train_times':[],\n",
    "                           'lr':[],\n",
    "                           'total_rtrn_pct':[]}\n",
    "    \n",
    "    data_len = len(df)\n",
    "    total_days = data_len\n",
    "    test_end = total_days\n",
    "    train_begin = 0\n",
    "    \n",
    "    pre_data = df.iloc[:, 4].values  # 取收盘价计算标签\n",
    "    label = []\n",
    "    for i in range(1, len(pre_data)):\n",
    "        label.append(round(pre_data[i] - pre_data[i - 1], 4))\n",
    "\n",
    "    df.loc[1:, 'label'] = label\n",
    "    df['label'] = df['label'].shift(-1)\n",
    "    df.fillna(method='ffill', inplace = True)\n",
    "    data = df.iloc[:, [1, 2, 3, 5]].values  # 获取特征量及标签，类型为np.ndarray\n",
    "    # logger.info(data[0:5])\n",
    "\n",
    "    \n",
    "    # 参数类别一：\n",
    "    \"\"\"\n",
    "    保持以下参数不变：\n",
    "\n",
    "    利用过去一天: 2018-11-16 0:00, 来预测下一天：\n",
    "    time_step = 20\n",
    "    rnn_unit = 10  # 隐藏层单元数量 ##################调整这个\n",
    "    input_size = 3  # 输入个数  \n",
    "    output_size = 1  # 输出个数\n",
    "    batch_size = 80  # 批量大小  #################调整这个\n",
    "    train_times = 20  # 训练次数\n",
    "    lr = 0.001  # 学习率\n",
    "\n",
    "\n",
    "    test_train_split 参数调优:\n",
    "\n",
    "    test_train_split   accuracy       信号强度\n",
    "           0.2           0.55         -55.86\n",
    "           0.3           0.52         -39.31\n",
    "           0.4           0.4928        -9.88\n",
    "           0.5           0.5111       -28.13\n",
    "           0.6           0.5          -13.87\n",
    "           0.7           0.4961       -27.13\n",
    "           0.8           0.4967       -28.58\n",
    "           0.9           0.5          -9.06\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    test_train_split = test_train_split\n",
    "    time_step = time_step # 20 as default\n",
    "    split_batch = int(total_days/time_step) \n",
    "    time_step_multiple = int(test_train_split*split_batch)\n",
    "    test_begin = int(test_end - (time_step)*time_step_multiple)\n",
    "    train_end = test_begin\n",
    "\n",
    "    # 这里 test_end - test_begin的差一定要是time_step的倍数\n",
    "#     print (\"split_batch: \", split_batch)\n",
    "#     print (\"train_test_split_rate: \", test_train_split)\n",
    "#     print (\"train_begin: \", train_begin)\n",
    "#     print (\"train_end: \", train_end)\n",
    "#     print (\"test_begin: \", test_begin)\n",
    "#     print (\"test_end: \", test_end)\n",
    "#     print (\"time_step: \", time_step)\n",
    "\n",
    "    # 参数类别二：\n",
    "\n",
    "    rnn_unit = rnn_unit # 1o as default  # 隐藏层单元数量 ##################调整这个\n",
    "    input_size = 3  # 输入个数  \n",
    "    output_size = 1  # 输出个数\n",
    "    batch_size = batch_size # 80 as default  # 批量大小  #################调整这个\n",
    "    train_times = train_times # 20 as default  # 训练次数\n",
    "    lr = lr # 0.001 as default  # 学习率\n",
    "    # 一般调整隐藏层数量、批量大小及学习率这几个超参数\n",
    "    # 输入和输出则由特征量和标签确定\n",
    "    # 本例中，以开盘价、最高价、最低价为特征量，\n",
    "    # 以收盘价差即涨跌作为标签\n",
    "    \n",
    "  \n",
    "\n",
    "    optimization_record = {'test_train_split': [],\n",
    "                           'time_step': [],\n",
    "                           'rnn_unit': [],\n",
    "                           'batch_size': [],\n",
    "                           'train_times':[],\n",
    "                           'lr':[],\n",
    "                           'total_rtrn_pct':[]}\n",
    "    \n",
    "    # Append参数：\n",
    "    \n",
    "    optimization_record['test_train_split'].append(test_train_split)\n",
    "    optimization_record['time_step'].append(time_step)\n",
    "    optimization_record['rnn_unit'].append(rnn_unit)\n",
    "    optimization_record['batch_size'].append(batch_size)\n",
    "    optimization_record['train_times'].append(train_times)\n",
    "    optimization_record['lr'].append(lr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print (\"=\"*60)\n",
    "    def get_train_data(batch_size, time_step, train_begin, train_end):\n",
    "        batch_index = []\n",
    "        data_train = data[train_begin + 1:train_end]\n",
    "        normalized_train_data = (\n",
    "            data_train - np.mean(data_train, axis=0)) / np.std(\n",
    "                data_train, axis=0)\n",
    "        # logger.info('normalized_train_data.shape: ', normalized_train_data.shape)\n",
    "        # logger.info(len(normalized_train_data))\n",
    "        # logger.info(type(normalized_train_data))\n",
    "        # logger.info(len([0, 2]))\n",
    "        # logger.info(normalized_train_data[0:20])\n",
    "        # logger.info(normalized_train_data[0:20, 1])\n",
    "\n",
    "        train_x, train_y = [], []\n",
    "        for i in range(len(normalized_train_data) - time_step):\n",
    "            if i % batch_size == 0:\n",
    "                batch_index.append(i)\n",
    "\n",
    "            x = normalized_train_data[i:i + time_step, :3]\n",
    "            y = normalized_train_data[i:i + time_step, 3, np.newaxis]\n",
    "            train_x.append(x.tolist())\n",
    "            train_y.append(y.tolist())\n",
    "\n",
    "        batch_index.append(len(normalized_train_data) - time_step)\n",
    "        return batch_index, train_x, train_y\n",
    "\n",
    "\n",
    "    def get_test_data(time_step, test_begin, test_end):\n",
    "        data_test = data[test_begin:test_end]\n",
    "        test_y = data_test[:, 3]\n",
    "        mean = np.mean(data_test, axis=0)\n",
    "        std = np.std(data_test, axis=0)\n",
    "        normalized_test_data = (data_test - mean) / std\n",
    "        size = (len(normalized_test_data) + time_step - 1) // time_step\n",
    "        test_x, test_y = [], []\n",
    "        for i in range(size - 1):\n",
    "            x = normalized_test_data[i * time_step:(i + 1) * time_step, :3]\n",
    "            y = normalized_test_data[i * time_step:(i + 1) * time_step, 3]\n",
    "            test_x.append(x.tolist())\n",
    "            test_y.extend(y)\n",
    "            # print('type(y): ', type(y))\n",
    "\n",
    "        test_x.append((normalized_test_data[(i + 1) * time_step:, :3]).tolist())\n",
    "        test_y.extend((normalized_test_data[(i + 1) * time_step:, 3]).tolist())\n",
    "        return mean, std, test_x, test_y\n",
    "\n",
    "\n",
    "    weights = {\n",
    "        'in': tf.Variable(tf.random_normal([input_size, rnn_unit])),\n",
    "        'out': tf.Variable(tf.random_normal([rnn_unit, 1]))\n",
    "    }\n",
    "    biases = {\n",
    "        'in': tf.Variable(tf.constant(0.1, shape=[\n",
    "            rnn_unit,\n",
    "        ])),\n",
    "        'out': tf.Variable(tf.constant(0.1, shape=[\n",
    "            1,\n",
    "        ]))\n",
    "    }\n",
    "\n",
    "\n",
    "    # 定义神经网络变量\n",
    "    def lstm(X):\n",
    "        batch_size = tf.shape(X)[0]\n",
    "        time_step = tf.shape(X)[1]\n",
    "        w_in = weights['in']\n",
    "        b_in = biases['in']\n",
    "        input_ = tf.reshape(X, [-1, input_size])\n",
    "        # print('input_.shape: ', input_.shape)\n",
    "        input_rnn = tf.matmul(input_, w_in) + b_in\n",
    "        # print('input_rnn.shape: ', input_rnn.shape)\n",
    "        # tensor转换成3维，作为cell的输入\n",
    "        input_rnn = tf.reshape(input_rnn, [-1, time_step, rnn_unit])\n",
    "        # print('input_rnn.shape: ', input_rnn.shape)\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_unit)\n",
    "        init_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        output_rnn, final_state = tf.nn.dynamic_rnn(\n",
    "            cell, input_rnn, initial_state=init_state, dtype=tf.float32)\n",
    "        output = tf.reshape(output_rnn, [-1, rnn_unit])\n",
    "        w_out = weights['out']\n",
    "        b_out = biases['out']\n",
    "        pred = tf.matmul(output, w_out) + b_out\n",
    "        return pred, final_state\n",
    "\n",
    "\n",
    "    # 训练模型\n",
    "    def train_lstm(batch_size, time_step, train_begin, train_end):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, time_step, input_size])\n",
    "        Y = tf.placeholder(tf.float32, shape=[None, time_step, output_size])\n",
    "        batch_index, train_x, train_y = get_train_data(batch_size, time_step,\n",
    "                                                       train_begin, train_end)\n",
    "        with tf.variable_scope('future_lstm'):\n",
    "            pred, _ = lstm(X)\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.square(tf.reshape(pred, [-1]) - tf.reshape(Y, [-1])))\n",
    "            train_operation = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=15)\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                for i in range(train_times):\n",
    "                    for step in range(len(batch_index) - 1):\n",
    "                        _, loss_ = sess.run(\n",
    "                            [train_operation, loss],\n",
    "                            feed_dict={\n",
    "                                X:\n",
    "                                train_x[batch_index[step]:batch_index[step + 1]],\n",
    "                                Y: train_y[batch_index[step]:batch_index[step + 1]]\n",
    "                            })\n",
    "                    # print('Number of iterations: {} , loss: {}'.format(i, loss_))\n",
    "#                     logger.info('Number of iterations: {} , loss: {}'.format(\n",
    "#                         i, loss_))\n",
    "    #             print('model saved: ', saver.save(sess, 'model_save1/model.ckpt'))\n",
    "                print('The train has finished')\n",
    "\n",
    "\n",
    "    train_lstm(batch_size, time_step, train_begin, train_end)\n",
    "\n",
    "\n",
    "    def prediction_and_evaluate(time_step):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, time_step, input_size])\n",
    "    #     print (\"X: \", X)  #brad's peek\n",
    "        mean, std, test_x, test_y = get_test_data(time_step, test_begin, test_end)\n",
    "        with tf.variable_scope('future_lstm', reuse=True):\n",
    "            pred, _ = lstm(X)\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            with tf.Session() as sess:\n",
    "                model_file = tf.train.latest_checkpoint('model_save1')\n",
    "                saver.restore(sess, model_file)\n",
    "                test_predict = []\n",
    "                # brad's peek:\n",
    "    #             print(\"test_x\")\n",
    "    #             print (test_x)\n",
    "                for step in range(len(test_x)):\n",
    "                    # 一次出time_step个结果，results.shape: (20, 1)\n",
    "                    results = sess.run(pred, feed_dict={X: [test_x[step]]})\n",
    "                    # predict = tf.reshape(results, [-1])\n",
    "                    predict = results.reshape(-1)\n",
    "                    test_predict.extend(predict)\n",
    "                # brad's peek:\n",
    "    #             print (test_predict)\n",
    "                # 还原真实值\n",
    "                # brad's peek:\n",
    "    #             print (\"test_y\",test_y)\n",
    "                test_y = np.array(test_y) * std[3] + mean[3]\n",
    "                test_predict = np.array(test_predict) * std[3] + mean[3]\n",
    "                true_y = test_y\n",
    "\n",
    "\n",
    "                # print('test_predict.shape: ', test_predict.shape)\n",
    "                # print('true_y.shape: ', true_y.shape)\n",
    "                out = np.c_[test_predict, true_y]\n",
    "                out_csv = pd.DataFrame(\n",
    "                    data=out, index=None, columns=['prediction', 'true'])\n",
    "                right, wrong = 0, 0\n",
    "                calc_data = map(lambda x, y: tuple((x, y)), test_predict,\n",
    "                                test_y[:len(test_predict)])\n",
    "                for data in calc_data:\n",
    "                    if data[0] * data[1] > 0 or data[0] == data[1]:\n",
    "                        right += 1\n",
    "                    else:\n",
    "                        wrong += 1\n",
    "                accuracy = right / (right + wrong)\n",
    "                print(\"利用过去一天: \"+str(df[:test_end].iloc[-1, :].values[0]) + \", 来预测下一天：\")\n",
    "                print (out_csv.iloc[-1, 0])\n",
    "                logger.info('The accuracy of this prediction: ')\n",
    "                logger.info(accuracy)\n",
    "        signal_file = out_csv.copy()\n",
    "        signal_file.rename(columns={\"true\": \"label\"}, inplace = True)\n",
    "        df_to_merge = df_asst_og[test_begin:].copy()\n",
    "        signal_file['time'] = df_to_merge['time'].values\n",
    "        merged = signal_file.merge(df_to_merge, on = 'time')\n",
    "    #     del merged['label_x']\n",
    "    #     del merged['label_y']\n",
    "        merged = merged[['time', 'open', 'high', 'low', 'close', 'prediction', 'label_x']]\n",
    "\n",
    "        def create_profits(x, y):\n",
    "            if x * y > 0:\n",
    "                return abs(y)\n",
    "            else:\n",
    "                return -abs(y)\n",
    "        merged['profits'] = np.vectorize(create_profits)(merged['prediction'], merged['label_x'])\n",
    "        merged['total_cum_profits'] = merged['profits'].cumsum()\n",
    "        merged['total_cum_profits'].plot(figsize = (18, 6))\n",
    "        return_pct = merged['total_cum_profits'].values[-1]/merged['close'].values[0]\n",
    "        return return_pct\n",
    "\n",
    "    target_to_optimize = prediction_and_evaluate(time_step)\n",
    "    optimization_record['total_rtrn_pct'].append(target_to_optimize)\n",
    "    print (\"total_return_percent: \", target_to_optimize)\n",
    "    return optimization_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be mainly used on local environment for optimization's purpose, not for exporting data to TB.\n",
    "\n",
    "def merge_and_optimize(asst, df, test_begin):\n",
    "    # 分析强度信号：\n",
    "    signal_loc = location\n",
    "    signal_file = pd.read_csv(signal_loc + \"pred_and_true_20181119_{}.csv\".format(asst), engine='python')\n",
    "    del signal_file['Unnamed: 0']\n",
    "    signal_file.rename(columns={\"true\": \"label\"}, inplace = True)\n",
    "    df_to_merge = df[test_begin:].copy()\n",
    "    signal_file['time'] = df_to_merge['time'].values\n",
    "    merged = signal_file.merge(df_to_merge, on = 'time')\n",
    "#     del merged['label_x']\n",
    "#     del merged['label_y']\n",
    "    merged = merged[['time', 'open', 'high', 'low', 'close', 'prediction', 'label_x']]\n",
    "    \n",
    "    def create_profits(x, y):\n",
    "        if x * y > 0:\n",
    "            return abs(y)\n",
    "        else:\n",
    "            return -abs(y)\n",
    "    merged['profits'] = np.vectorize(create_profits)(merged['prediction'], merged['label_x'])\n",
    "    merged['total_cum_profits'] = merged['profits'].cumsum()\n",
    "#     merged['total_cum_profits'].plot(figsize = (18, 6))\n",
    "    return_pct = merged['total_cum_profits'].values[-1]/merged['close'].values[0]\n",
    "    return return_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_to_optimize = {'test_train_split': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "                     'time_step': [20, 40],\n",
    "                     'rnn_unit': [10, 20],\n",
    "                     'batch_size': [],\n",
    "                     'train_times':[],\n",
    "                     'lr':[],\n",
    "                     'total_rtrn_pct':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "The train has finished\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "Skipping the error paras\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "C:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "C:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "C:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:130: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train has finished\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "============================================================\n",
      "The train has finished\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint model_save1\\model.ckpt\n",
      "Skipping the error paras\n",
      "47.24364352226257\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "results_list = []\n",
    "for tts in paras_to_optimize['test_train_split']:\n",
    "    for ts in paras_to_optimize['time_step']:\n",
    "        for ru in paras_to_optimize['rnn_unit']:\n",
    "            try:\n",
    "                r = lstm_all_in_one_optimization(df_trx, df_trx, 'trx', \n",
    "                                             test_train_split = tts, \n",
    "                                             time_step = ts, \n",
    "                                             rnn_unit = ru, \n",
    "                                             batch_size = 80, \n",
    "                                             train_times = 20, \n",
    "                                             lr = 0.001)\n",
    "                results_list.append(r)\n",
    "\n",
    "            except:\n",
    "                print (\"Skipping the error paras\")\n",
    "            \n",
    "            \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'total_rtrn_pct'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d1bff06e7650>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'total_rtrn_pct'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position)\u001b[0m\n\u001b[0;32m   4419\u001b[0m             \u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4420\u001b[0m             k = self._get_label_or_level_values(by, axis=axis,\n\u001b[1;32m-> 4421\u001b[1;33m                                                 stacklevel=stacklevel)\n\u001b[0m\u001b[0;32m   4422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4423\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis, stacklevel)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'total_rtrn_pct'"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results_list)\n",
    "df_results.sort_values('total_rtrn_pct', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results.to_csv(location+\"trx_paras_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
