{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/bradsun91/alpha-scientist/blob/master/content/05_Model_Scoring.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Use of machine learning in the quantitative investment field is, by all indications, skyrocketing. The proliferation of easily accessible data - both traditional and alternative - along with some very approachable frameworks for machine learning models - is encouraging many to explore the arena.\n",
    "\n",
    "However, these financial ML explorers are learning that there are many ways in which using ML to predict financial time series differs greatly from labeling cat pictures or flagging spam. Among these differences is that traditional model performance metrics (RSQ, MSE, accuracy, F1, etc...) can be misleading and incomplete.\n",
    "\n",
    "Over the past several years, I've developed a set of metrics which have proved useful for comparing and optimizing financial time series models. These metrics attempt to measure models' predictive power but also their trade-ability, critically important for those who actually intend to use their models in the real world.\n",
    "\n",
    "In this post, I will present a general outline of my approach and will demonstrate a few of the most useful metrics I've added to my standard \"scorecard\". I look forward to hearing how others may think to extend the concept. If you'd like to replicate and experiment with the below code, you can download the source notebook for this post by right-clicking on the below button and choosing \"save link as\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't already checked out the previous four installments in this tutorial, you may want review those first. Many of the coding patterns used below are discussed at length:\n",
    "\n",
    "- Part 1: Data Management\n",
    "- Part 2: Feature Engineering\n",
    "- Part 3: Feature Selection\n",
    "- Part 4: Walk-forward model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"/Users/miaoyuesun/Code_Workspace/brad_public_workspace_mac/data/\" \n",
    "j = \"j9000.csv\"\n",
    "jm = \"jm000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_df = pd.read_csv(location+j, engine=\"python\", header=None)\n",
    "jm_df = pd.read_csv(location+jm, engine='python', header=None)\n",
    "j_df.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'holdings']\n",
    "jm_df.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'holdings']\n",
    "\n",
    "jm_df_cols = jm_df[['date', 'open', 'high', 'low', 'close', 'volume']]\n",
    "j_df_cols = j_df[['date', 'open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "jm_df_cols['symbol'] = 'jm'\n",
    "j_df_cols['symbol'] = 'j'\n",
    "\n",
    "jm_df_cols['date'] = pd.to_datetime(jm_df_cols['date'])\n",
    "j_df_cols['date'] = pd.to_datetime(j_df_cols['date'])\n",
    "\n",
    "jm_df_cols = jm_df_cols.set_index(['date','symbol'])\n",
    "j_df_cols = j_df_cols.set_index(['date', 'symbol'])\n",
    "\n",
    "prices = pd.concat([jm_df_cols, j_df_cols]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-04-15 09:00:00</th>\n",
       "      <th>j</th>\n",
       "      <td>2275.0</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>2260.0</td>\n",
       "      <td>31834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-15 10:00:00</th>\n",
       "      <th>j</th>\n",
       "      <td>2259.0</td>\n",
       "      <td>2265.0</td>\n",
       "      <td>2252.0</td>\n",
       "      <td>2256.0</td>\n",
       "      <td>6440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-15 11:00:00</th>\n",
       "      <th>j</th>\n",
       "      <td>2256.0</td>\n",
       "      <td>2258.0</td>\n",
       "      <td>2228.0</td>\n",
       "      <td>2248.0</td>\n",
       "      <td>8264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-15 13:00:00</th>\n",
       "      <th>j</th>\n",
       "      <td>2246.0</td>\n",
       "      <td>2251.0</td>\n",
       "      <td>2238.0</td>\n",
       "      <td>2240.0</td>\n",
       "      <td>4832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-15 14:00:00</th>\n",
       "      <th>j</th>\n",
       "      <td>2241.0</td>\n",
       "      <td>2252.0</td>\n",
       "      <td>2239.0</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>6710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              open    high     low   close  volume\n",
       "date                symbol                                        \n",
       "2011-04-15 09:00:00 j       2275.0  2285.0  2250.0  2260.0   31834\n",
       "2011-04-15 10:00:00 j       2259.0  2265.0  2252.0  2256.0    6440\n",
       "2011-04-15 11:00:00 j       2256.0  2258.0  2228.0  2248.0    8264\n",
       "2011-04-15 13:00:00 j       2246.0  2251.0  2238.0  2240.0    4832\n",
       "2011-04-15 14:00:00 j       2241.0  2252.0  2239.0  2250.0    6710"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code generates several features then synthetically generates an outcome series from them (along with noise). This guarantees that the features will be informative, since the outcome has been constructed to ensure a relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miaoyuesun/anaconda3/lib/python3.5/site-packages/pandas/compat/_optional.py:106: UserWarning: Pandas requires version '2.6.2' or newer of 'numexpr' (version '2.6.1' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "num_obs = prices.close.count()\n",
    "\n",
    "def add_memory(s,n_days=50,mem_strength=0.1):\n",
    "    ''' adds autoregressive behavior to series of data'''\n",
    "    add_ewm = lambda x: (1-mem_strength)*x + mem_strength*x.ewm(n_days).mean()\n",
    "    out = s.groupby(level='symbol').apply(add_ewm)\n",
    "    return out\n",
    "\n",
    "# generate feature data\n",
    "f01 = pd.Series(np.random.randn(num_obs),index=prices.index)\n",
    "f01 = add_memory(f01,10,0.1)\n",
    "f02 = pd.Series(np.random.randn(num_obs),index=prices.index)\n",
    "f02 = add_memory(f02,10,0.1)\n",
    "f03 = pd.Series(np.random.randn(num_obs),index=prices.index)\n",
    "f03 = add_memory(f03,10,0.1)\n",
    "f04 = pd.Series(np.random.randn(num_obs),index=prices.index)\n",
    "f04 = f04 # no memory\n",
    "\n",
    "features = pd.concat([f01,f02,f03,f04],axis=1)\n",
    "\n",
    "## now, create response variable such that it is related to features\n",
    "# f01 becomes increasingly important, f02 becomes decreasingly important,\n",
    "# f03 oscillates in importance, f04 is stationary, \n",
    "# and finally a noise component is added\n",
    "\n",
    "outcome =   f01 * np.linspace(0.5,1.5,num_obs) + \\\n",
    "            f02 * np.linspace(1.5,0.5,num_obs) + \\\n",
    "            f03 * pd.Series(np.sin(2*np.pi*np.linspace(0,1,num_obs)*2)+1,index=f03.index) + \\\n",
    "            f04 + \\\n",
    "            np.random.randn(num_obs) * 3 \n",
    "outcome.name = 'outcome'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating models and predictions\n",
    "\n",
    "Imagine that we created a simple linear model (such as below) and wanted to measure its effectiveness at prediction.\n",
    "\n",
    "Note: we'll follow the walk-forward modeling process described in the previous post. If you don't understand the below code snippet (and want to...) please check out that post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## fit models for each timestep on a walk-forward basis\n",
    "recalc_dates = features.resample('Q',level='date').mean().index.values[:-1]\n",
    "models = pd.Series(index=recalc_dates)\n",
    "for date in recalc_dates:\n",
    "    X_train = features.xs(slice(None,date),level='date',drop_level=False)\n",
    "    y_train = outcome.xs(slice(None,date),level='date',drop_level=False)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    models.loc[date] = model\n",
    "\n",
    "## predict values walk-forward (all predictions out of sample)\n",
    "begin_dates = models.index\n",
    "end_dates = models.index[1:].append(pd.to_datetime(['2099-12-31']))\n",
    "\n",
    "predictions = pd.Series(index=features.index)\n",
    "\n",
    "for i,model in enumerate(models): #loop thru each models object in collection\n",
    "    X = features.xs(slice(begin_dates[i],end_dates[i]),level='date',drop_level=False)\n",
    "    p = pd.Series(model.predict(X),index=X.index)\n",
    "    predictions.loc[X.index] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2011-09-30', '2011-12-31', '2012-03-31', '2012-06-30',\n",
       "               '2012-09-30', '2012-12-31', '2013-03-31', '2013-06-30',\n",
       "               '2013-09-30', '2013-12-31', '2014-03-31', '2014-06-30',\n",
       "               '2014-09-30', '2014-12-31', '2015-03-31', '2015-06-30',\n",
       "               '2015-09-30', '2015-12-31', '2016-03-31', '2016-06-30',\n",
       "               '2016-09-30', '2016-12-31', '2017-03-31', '2017-06-30',\n",
       "               '2017-09-30', '2017-12-31', '2018-03-31', '2018-06-30',\n",
       "               '2018-09-30', '2099-12-31'],\n",
       "              dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional model evaluation\n",
    "\n",
    "So we've got a model, we've got a sizeable set of (out of sample) predictions. Is the model any good? Should we junk it, tune it, or trade it? Since this is a regression model, I'll throw our data into scikit-learn's metrics package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance    0.263251\n",
      "MAE                   2.478018\n",
      "MSE                   9.646065\n",
      "MedAE                 2.098443\n",
      "RSQ                   0.263251\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "# make sure we have 1-for-1 mapping between pred and true\n",
    "# think of outcome as a fake series of stock prices - Brad\n",
    "common_idx = outcome.dropna().index.intersection(predictions.dropna().index)\n",
    "\n",
    "# y_true: just like fake stock prices - Brad\n",
    "y_true = outcome[common_idx]\n",
    "y_true.name = 'y_true'\n",
    "# \n",
    "y_pred = predictions[common_idx]\n",
    "y_pred.name = 'y_pred'\n",
    "\n",
    "standard_metrics = pd.Series()\n",
    "\n",
    "standard_metrics.loc['explained variance'] = metrics.explained_variance_score(y_true, y_pred)\n",
    "standard_metrics.loc['MAE'] = metrics.mean_absolute_error(y_true, y_pred)\n",
    "standard_metrics.loc['MSE'] = metrics.mean_squared_error(y_true, y_pred)\n",
    "standard_metrics.loc['MedAE'] = metrics.median_absolute_error(y_true, y_pred)\n",
    "standard_metrics.loc['RSQ'] = metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "print(standard_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These stats don't really tell us much by themselves. You may have an intuition for r-squared so that may give you a level of confidence in the models. However, even this metric has problems not to mention does not tell us much about the practicality of this signal from a trading point of view.\n",
    "\n",
    "True, we could construct some trading rules around this series of predictions and perform a formal backtest on that. However, that is quite time consuming and introduces a number of extraneous variables into the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A better way... Creating custom metrics\n",
    "\n",
    "Instead of relying on generic ML metrics, we will create several custom metrics that will hopefully give a more complete picture of strength, reliability, and practicality of these models.\n",
    "\n",
    "I'll work through an example of creating an extensible scorecard with about a half dozen custom-defined metrics as a starting point. You can feel free to extend this into a longer scorecard which is suited to your needs and beliefs. In my own trading, I use about 25 metrics in a standard \"scorecard\" each time I evaluate a model. You may prefer to use more, fewer, or different metrics but the process should be applicable.\n",
    "\n",
    "I'll focus only on regression-oriented metrics (i.e., those which use a continuous prediction rather than a binary or classification prediction). It's trivial to re-purpose the same framework to a classification-oriented environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Preprocess data primitives\n",
    "\n",
    "Before implementing specific metrics we need to do some data pre-processing. It'll become clear why doing this first will save considerable time later when calculating aggregate metrics.\n",
    "\n",
    "\n",
    "To create these intermediate values, you'll need the following inputs:\n",
    "\n",
    "\n",
    "- y_pred: the continuous variable prediction made by your model for each timestep, for each symbol\n",
    "- y_true: the continuous variable actual outcome for each timestep, for each symbol.\n",
    "- index: this is the unique identifier for each prediction or actual result. If working with a single instrument, then you can simply use date (or time or whatever). If you're using multiple instruments, a multi-index with (date/symbol) is necessary.\n",
    "\n",
    "\n",
    "In other words, if your model is predicting one-day price changes, you'd want your y_pred to be the model's predictions made as of March 9th (for the coming day), indexed as 2017-03-09 and you'd want the actual future outcome which will play out in the next day also aligned to Mar 9th. This \"peeking\" convention is very useful for working with large sets of data across different time horizons. It is described ad nauseum in Part 1: Data Management.\n",
    "\n",
    "The raw input data we need to provide might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     y_pred    y_true\n",
      "date       symbol                    \n",
      "2018-12-26 jm      0.210931  5.761904\n",
      "2018-12-27 j      -0.497826 -5.743094\n",
      "           jm     -0.613546  3.997778\n",
      "2018-12-28 j      -0.546005  2.321911\n",
      "           jm      1.128044 -2.139567\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat([y_pred,y_true],axis=1).tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will feed this data into a simple function which will return a dataframe with the y_pred and y_true values, along with several other useful derivative values. These derivative values include:\n",
    "\n",
    "- sign_pred: positive or negative sign of prediction\n",
    "- sign_true: positive or negative sign of true outcome\n",
    "- is_correct: 1 if sign_pred == sign_true, else 0\n",
    "- is_incorrect: opposite\n",
    "- is_predicted: 1 if the model has made a valid prediction, 0 if not. This is important if models only emit predictions when they have a certain level of confidence\n",
    "- result: the profit (loss) resulting from betting one unit in the direction of the sign_pred. This is the continuous variable result of following the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              y_pred    y_true  sign_pred  sign_true  \\\n",
      "date                symbol                                             \n",
      "2016-02-25 22:00:00 j      -1.381706 -2.701665       -1.0       -1.0   \n",
      "2016-07-20 09:00:00 jm     -2.836265 -5.898601       -1.0       -1.0   \n",
      "2011-07-01 11:00:00 j       0.056496 -1.300877        1.0       -1.0   \n",
      "2015-11-03 22:00:00 jm      0.269227 -3.390068        1.0       -1.0   \n",
      "2018-11-28 11:00:00 j       0.113279  2.230594        1.0        1.0   \n",
      "\n",
      "                            is_correct  is_incorrect  is_predicted    result  \n",
      "date                symbol                                                    \n",
      "2016-02-25 22:00:00 j                1             0             1  2.701665  \n",
      "2016-07-20 09:00:00 jm               1             0             1  5.898601  \n",
      "2011-07-01 11:00:00 j                0             1             1 -1.300877  \n",
      "2015-11-03 22:00:00 jm               0             1             1 -3.390068  \n",
      "2018-11-28 11:00:00 j                1             0             1  2.230594  \n"
     ]
    }
   ],
   "source": [
    "def make_df(y_pred,y_true):\n",
    "    y_pred.name = 'y_pred'\n",
    "    y_true.name = 'y_true'\n",
    "    \n",
    "    df = pd.concat([y_pred,y_true],axis=1)\n",
    "\n",
    "    df['sign_pred'] = df.y_pred.apply(np.sign)\n",
    "    df['sign_true'] = df.y_true.apply(np.sign)\n",
    "    df['is_correct'] = 0\n",
    "    df.loc[df.sign_pred * df.sign_true > 0 ,'is_correct'] = 1 # only registers 1 when prediction was made AND it was correct\n",
    "    df['is_incorrect'] = 0\n",
    "    df.loc[df.sign_pred * df.sign_true < 0,'is_incorrect'] = 1 # only registers 1 when prediction was made AND it was wrong\n",
    "    df['is_predicted'] = df.is_correct + df.is_incorrect\n",
    "    df['result'] = df.sign_pred * df.y_true \n",
    "    return df\n",
    "\n",
    "df = make_df(y_pred,y_true)\n",
    "print(df.dropna().tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_true</th>\n",
       "      <th>sign_pred</th>\n",
       "      <th>sign_true</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>is_incorrect</th>\n",
       "      <th>is_predicted</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-09-15 22:00:00</th>\n",
       "      <th>jm</th>\n",
       "      <td>0.988718</td>\n",
       "      <td>3.391933</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.391933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-24 23:00:00</th>\n",
       "      <th>j</th>\n",
       "      <td>-1.790059</td>\n",
       "      <td>2.008035</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.008035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-25 14:00:00</th>\n",
       "      <th>j</th>\n",
       "      <td>-3.325028</td>\n",
       "      <td>-1.058895</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.058895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-13 13:00:00</th>\n",
       "      <th>j</th>\n",
       "      <td>0.345094</td>\n",
       "      <td>2.259794</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.259794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-17 13:00:00</th>\n",
       "      <th>jm</th>\n",
       "      <td>1.359469</td>\n",
       "      <td>1.998052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.998052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-25 22:00:00</th>\n",
       "      <th>j</th>\n",
       "      <td>-1.381706</td>\n",
       "      <td>-2.701665</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.701665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-20 09:00:00</th>\n",
       "      <th>jm</th>\n",
       "      <td>-2.836265</td>\n",
       "      <td>-5.898601</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.898601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-01 11:00:00</th>\n",
       "      <th>j</th>\n",
       "      <td>0.056496</td>\n",
       "      <td>-1.300877</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.300877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-03 22:00:00</th>\n",
       "      <th>jm</th>\n",
       "      <td>0.269227</td>\n",
       "      <td>-3.390068</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.390068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-28 11:00:00</th>\n",
       "      <th>j</th>\n",
       "      <td>0.113279</td>\n",
       "      <td>2.230594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.230594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22886 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              y_pred    y_true  sign_pred  sign_true  \\\n",
       "date                symbol                                             \n",
       "2017-09-15 22:00:00 jm      0.988718  3.391933        1.0        1.0   \n",
       "2015-11-24 23:00:00 j      -1.790059  2.008035       -1.0        1.0   \n",
       "2015-11-25 14:00:00 j      -3.325028 -1.058895       -1.0       -1.0   \n",
       "2016-10-13 13:00:00 j       0.345094  2.259794        1.0        1.0   \n",
       "2018-07-17 13:00:00 jm      1.359469  1.998052        1.0        1.0   \n",
       "...                              ...       ...        ...        ...   \n",
       "2016-02-25 22:00:00 j      -1.381706 -2.701665       -1.0       -1.0   \n",
       "2016-07-20 09:00:00 jm     -2.836265 -5.898601       -1.0       -1.0   \n",
       "2011-07-01 11:00:00 j       0.056496 -1.300877        1.0       -1.0   \n",
       "2015-11-03 22:00:00 jm      0.269227 -3.390068        1.0       -1.0   \n",
       "2018-11-28 11:00:00 j       0.113279  2.230594        1.0        1.0   \n",
       "\n",
       "                            is_correct  is_incorrect  is_predicted    result  \n",
       "date                symbol                                                    \n",
       "2017-09-15 22:00:00 jm               1             0             1  3.391933  \n",
       "2015-11-24 23:00:00 j                0             1             1 -2.008035  \n",
       "2015-11-25 14:00:00 j                1             0             1  1.058895  \n",
       "2016-10-13 13:00:00 j                1             0             1  2.259794  \n",
       "2018-07-17 13:00:00 jm               1             0             1  1.998052  \n",
       "...                                ...           ...           ...       ...  \n",
       "2016-02-25 22:00:00 j                1             0             1  2.701665  \n",
       "2016-07-20 09:00:00 jm               1             0             1  5.898601  \n",
       "2011-07-01 11:00:00 j                0             1             1 -1.300877  \n",
       "2015-11-03 22:00:00 jm               0             1             1 -3.390068  \n",
       "2018-11-28 11:00:00 j                1             0             1  2.230594  \n",
       "\n",
       "[22886 rows x 8 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our metrics\n",
    "\n",
    "With this set of intermediate variables pre-processed, we can more easily calculate metrics. The metrics we'll start with here include things like:\n",
    "\n",
    "- Accuracy: Just as the name suggests, this measures the percent of predictions that were directionally correct vs. incorrect.\n",
    "\n",
    "- Edge: perhaps the most useful of all metrics, this is the expected value of the prediction over a sufficiently large set of draws. Think of this like a blackjack card counter who knows the expected profit on each dollar bet when the odds are at a level of favorability\n",
    "\n",
    "- Noise: critically important but often ignored, the noise metric estimates how dramatically the model's predictions vary from one day to the next. As you might imagine, a model which abruptly changes its mind every few days is much harder to follow (and much more expensive to trade) than one which is a bit more steady.\n",
    "\n",
    "The below function takes in our pre-processed data primitives and returns a scorecard with accuracy, edge, and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy    66.089645\n",
       "edge         1.450701\n",
       "noise        2.280260\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_scorecard(df):\n",
    "    scorecard = pd.Series()\n",
    "    # building block metrics\n",
    "    scorecard.loc['accuracy'] = df.is_correct.sum()*1. / (df.is_predicted.sum()*1.)*100\n",
    "    scorecard.loc['edge'] = df.result.mean()\n",
    "    scorecard.loc['noise'] = df.y_pred.diff().abs().mean()\n",
    "    \n",
    "    return scorecard    \n",
    "\n",
    "calc_scorecard(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. I now know that we've been directionally correct about two-thirds of the time, and that following this signal would create an edge of ~1.5 units per time period.\n",
    "\n",
    "Let's keep going. We can now easily combine and transform things to derive new metrics. The below function shows several examples, including:\n",
    "\n",
    "- y_true_chg and y_pred_chg: The average magnitude of change (per period) in y_true and y_pred.\n",
    "- prediction_calibration: A simple ratio of the magnitude of our predictions vs. magnitude of truth. This gives some indication of whether our model is properly tuned to the size of movement in addition to the direction of it.\n",
    "- capture_ratio: Ratio of the \"edge\" we gain by following our predictions vs. the actual daily change. 100 would indicate that we were perfectly capturing the true movement of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy                  66.089645\n",
       "edge                       1.450701\n",
       "noise                      2.280260\n",
       "y_true_chg                 2.860478\n",
       "y_pred_chg                 1.617638\n",
       "prediction_calibration     0.565513\n",
       "capture_ratio             50.715321\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_scorecard(df):\n",
    "    scorecard = pd.Series()\n",
    "    # building block metrics\n",
    "    scorecard.loc['accuracy'] = df.is_correct.sum()*1. / (df.is_predicted.sum()*1.)*100\n",
    "    scorecard.loc['edge'] = df.result.mean()\n",
    "    scorecard.loc['noise'] = df.y_pred.diff().abs().mean()\n",
    "\n",
    "    # derived metrics\n",
    "    scorecard.loc['y_true_chg'] = df.y_true.abs().mean()\n",
    "    scorecard.loc['y_pred_chg'] = df.y_pred.abs().mean()\n",
    "    scorecard.loc['prediction_calibration'] = scorecard.loc['y_pred_chg']/scorecard.loc['y_true_chg']\n",
    "    scorecard.loc['capture_ratio'] = scorecard.loc['edge']/scorecard.loc['y_true_chg']*100\n",
    "\n",
    "    return scorecard    \n",
    "\n",
    "calc_scorecard(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, metrics can be easily calculated for only long or short predictions (for a two-sided model) or separately for positions which ended up being winners and losers.\n",
    "\n",
    "- edge_long and edge_short: The \"edge\" for only long signals or for short signals.\n",
    "- edge_win and edge_lose: The \"edge\" for only winners or for only losers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy                  66.089645\n",
       "edge                       1.450701\n",
       "noise                      2.280260\n",
       "y_true_chg                 2.860478\n",
       "y_pred_chg                 1.617638\n",
       "prediction_calibration     0.565513\n",
       "capture_ratio             50.715321\n",
       "edge_long                  1.514464\n",
       "edge_short                 1.530197\n",
       "edge_win                   3.333592\n",
       "edge_lose                 -2.006705\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_scorecard(df):\n",
    "    scorecard = pd.Series()\n",
    "    # building block metrics\n",
    "    scorecard.loc['accuracy'] = df.is_correct.sum()*1. / (df.is_predicted.sum()*1.)*100\n",
    "    scorecard.loc['edge'] = df.result.mean()\n",
    "    scorecard.loc['noise'] = df.y_pred.diff().abs().mean()\n",
    "\n",
    "    # derived metrics\n",
    "    scorecard.loc['y_true_chg'] = df.y_true.abs().mean()\n",
    "    scorecard.loc['y_pred_chg'] = df.y_pred.abs().mean()\n",
    "    scorecard.loc['prediction_calibration'] = scorecard.loc['y_pred_chg']/scorecard.loc['y_true_chg']\n",
    "    scorecard.loc['capture_ratio'] = scorecard.loc['edge']/scorecard.loc['y_true_chg']*100\n",
    "\n",
    "    # metrics for a subset of predictions\n",
    "    scorecard.loc['edge_long'] = df[df.sign_pred == 1].result.mean()  - df.y_true.mean()\n",
    "    scorecard.loc['edge_short'] = df[df.sign_pred == -1].result.mean()  - df.y_true.mean()\n",
    "\n",
    "    scorecard.loc['edge_win'] = df[df.is_correct == 1].result.mean()  - df.y_true.mean()\n",
    "    scorecard.loc['edge_lose'] = df[df.is_incorrect == 1].result.mean()  - df.y_true.mean()\n",
    "\n",
    "    return scorecard    \n",
    "\n",
    "calc_scorecard(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this slate of metrics, we've gained much more insight than we got from MSE, R-squared, etc...\n",
    "\n",
    "- The model is predicting with a strong directional accuracy\n",
    "- We are generating about 1.4 units of \"edge\" (expected profit) each prediction, which is about half of the total theoretical profit\n",
    "- The model makes more on winners than it loses on losers\n",
    "- The model is equally valid on both long and short predictions\n",
    "\n",
    "If this were real data, I would be rushing to put this model into production!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics over time\n",
    "\n",
    "Critically important when considering using a model in live trading is to understand (a) how consistent the model's performance has been, and (b) whether its current performance has degraded from its past. Markets have a way of discovering and eliminating past sources of edge.\n",
    "\n",
    "Here, a two line function will calculate each metric by year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year                         2011       2012       2013       2014       2015  \\\n",
      "accuracy                74.603175  67.901235  66.197183  63.265306  65.368852   \n",
      "edge                     2.099356   1.852989   1.568403   0.992050   1.406475   \n",
      "noise                    2.547492   2.552125   2.571957   2.373444   2.231143   \n",
      "y_true_chg               2.950821   2.995328   2.831913   2.762163   2.838464   \n",
      "y_pred_chg               1.916061   1.849373   1.810558   1.681045   1.573468   \n",
      "prediction_calibration   0.649332   0.617419   0.639341   0.608597   0.554338   \n",
      "capture_ratio           71.144815  61.862645  55.383166  35.915705  49.550579   \n",
      "edge_long                2.300789   1.935861   1.734747   1.000525   1.467651   \n",
      "edge_short               2.272138   3.185282   1.190485   0.909155   1.562350   \n",
      "edge_win                 3.570436   4.321410   3.193581   2.929456   3.357573   \n",
      "edge_lose               -1.490579  -1.028126  -1.999003  -2.446903  -1.956827   \n",
      "\n",
      "year                         2016       2017       2018  \n",
      "accuracy                65.573770  68.237705  64.814815  \n",
      "edge                     1.612960   1.428995   1.343915  \n",
      "noise                    2.013979   2.110842   2.207332  \n",
      "y_true_chg               2.857985   2.950581   2.827928  \n",
      "y_pred_chg               1.456685   1.497341   1.518131  \n",
      "prediction_calibration   0.509689   0.507473   0.536835  \n",
      "capture_ratio           56.436968  48.430976  47.522966  \n",
      "edge_long                1.641972   1.392507   1.509646  \n",
      "edge_short               2.028413   1.280016   1.139218  \n",
      "edge_win                 3.635288   3.117708   3.178978  \n",
      "edge_lose               -1.582059  -2.486621  -2.148160  \n"
     ]
    }
   ],
   "source": [
    "def scorecard_by_year(df):\n",
    "    df['year'] = df.index.get_level_values('date').year\n",
    "    return df.groupby('year').apply(calc_scorecard).T\n",
    "\n",
    "print(scorecard_by_year(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's just as simple to compare performance across symbols (or symbol groups, if you've defined those):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol                          j         jm\n",
      "accuracy                65.790914  66.477273\n",
      "edge                     1.471711   1.423438\n",
      "noise                    2.245444   2.239948\n",
      "y_true_chg               2.825245   2.906197\n",
      "y_pred_chg               1.632910   1.597822\n",
      "prediction_calibration   0.577971   0.549798\n",
      "capture_ratio           52.091462  48.979401\n",
      "edge_long                1.475656   1.564949\n",
      "edge_short               1.535812   1.527808\n",
      "edge_win                 3.299720   3.377598\n",
      "edge_lose               -1.944219  -2.090452\n"
     ]
    }
   ],
   "source": [
    "def scorecard_by_symbol(df):\n",
    "    return df.groupby(level='symbol').apply(calc_scorecard).T\n",
    "\n",
    "print(scorecard_by_symbol(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing models\n",
    "\n",
    "The added insight we get from this methodology comes when wanting to make comparisons between models, periods, segments, etc...\n",
    "\n",
    "To illustrate, let's say that we're comparing two models, a linear regression vs. a random forest, for performance on a training set and a testing set (pretend for a moment that we didn't adhere to Walk-forward model building practices...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        model1_train  model1_test  model2_train  model2_test\n",
      "accuracy                   66.945607    66.261398     88.817041    62.765957\n",
      "edge                        1.515663     1.418232      2.670492     1.190708\n",
      "noise                       2.078112     2.253423      3.085657     2.646271\n",
      "y_true_chg                  2.870653     2.855090      2.870653     2.855090\n",
      "y_pred_chg                  1.476714     1.582451      2.200979     1.894188\n",
      "prediction_calibration      0.514417     0.554256      0.766717     0.663442\n",
      "capture_ratio              52.798548    49.673816     93.027315    41.704741\n",
      "edge_long                   1.544550     1.562990      2.673415     1.308844\n",
      "edge_short                  1.706784     1.151250      2.890599     0.945018\n",
      "edge_win                    3.387669     3.145336      3.231055     3.143665\n",
      "edge_lose                  -1.937998    -2.208656     -0.783301    -2.314286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNetCV,Lasso,Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(features,outcome,test_size=0.20,shuffle=False)\n",
    "\n",
    "# linear regression\n",
    "model1 = LinearRegression().fit(X_train,y_train)\n",
    "model1_train = pd.Series(model1.predict(X_train),index=X_train.index)\n",
    "model1_test = pd.Series(model1.predict(X_test),index=X_test.index)\n",
    "\n",
    "model2 = RandomForestRegressor().fit(X_train,y_train)\n",
    "model2_train = pd.Series(model2.predict(X_train),index=X_train.index)\n",
    "model2_test = pd.Series(model2.predict(X_test),index=X_test.index)\n",
    "\n",
    "# create dataframes for each \n",
    "model1_train_df = make_df(model1_train,y_train)\n",
    "model1_test_df = make_df(model1_test,y_test)\n",
    "model2_train_df = make_df(model2_train,y_train)\n",
    "model2_test_df = make_df(model2_test,y_test)\n",
    "\n",
    "s1 = calc_scorecard(model1_train_df)\n",
    "s1.name = 'model1_train'\n",
    "s2 = calc_scorecard(model1_test_df)\n",
    "s2.name = 'model1_test'\n",
    "s3 = calc_scorecard(model2_train_df)\n",
    "s3.name = 'model2_train'\n",
    "s4 = calc_scorecard(model2_test_df)\n",
    "s4.name = 'model2_test'\n",
    "\n",
    "print(pd.concat([s1,s2,s3,s4],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quick and dirty scorecard comparison gives us a great deal of useful information. We learn that:\n",
    "\n",
    "- The relatively simple linear regression (model1) does a very good job of prediction, correct about 68% of the time, capturing >50% of available price movement (this is very good) during training\n",
    "- Model1 holds up very well out of sample, performing nearly as well on test as train\n",
    "- Model2, a more complex random forest ensemble model, appears far superior on the training data, capturing 90%+ of available price action, but appears quite overfit and does not perform nearly as well on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this tutorial, we've covered a framework for evaluating models in a market prediction context and have demonstrated a few useful metrics. However, the approach can be extended much further to suit your needs. You can consider:\n",
    "\n",
    "- Adding new metrics to the standard scorecard\n",
    "- Comparing scorecard metrics for subsets of the universe. For instance, each symbol or grouping of symbols\n",
    "- Calculating and plotting performance metrics across time to validate robustness or to identify trends\n",
    "\n",
    "In the final post of this series, I'll present a unique framework for creating an ensemble model to blend together the results of your many different forecasting models.\n",
    "\n",
    "Please feel free to add to the comment section with your good ideas for useful metrics, with questions/comments on this post, and topic ideas for future posts.\n",
    "\n",
    "# One last thing...\n",
    "\n",
    "If you've found this post useful, please follow @data2alpha on twitter and forward to a friend or colleague who may also find this topic interesting.\n",
    "\n",
    "Finally, take a minute to leave a comment below - either to discuss this post or to offer an idea for future posts. Thanks for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
