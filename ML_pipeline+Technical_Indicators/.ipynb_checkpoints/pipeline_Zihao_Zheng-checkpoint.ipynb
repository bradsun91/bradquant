{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tfm2ahV-kCPM"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (wrapper.py, line 46)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/miaoyuesun/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2961\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-1-0a2df533ea60>\"\u001b[0m, line \u001b[1;32m11\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    import ta\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/miaoyuesun/anaconda3/lib/python3.5/site-packages/ta/__init__.py\"\u001b[0;36m, line \u001b[0;32m8\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from ta.wrapper import (add_all_ta_features, add_momentum_ta, add_others_ta,\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/miaoyuesun/anaconda3/lib/python3.5/site-packages/ta/wrapper.py\"\u001b[0;36m, line \u001b[0;32m46\u001b[0m\n\u001b[0;31m    df[f'{colprefix}volume_adi'] = AccDistIndexIndicator(\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.signal import argrelextrema\n",
    "from collections import defaultdict\n",
    "# import pandas_datareader.data as web\n",
    "import ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UeoELiqKkAPm"
   },
   "outputs": [],
   "source": [
    "class PipeLine(object):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \"\"\" \n",
    "        features --> Open, High, Low, Close, Volume, news, P/E ratio,\n",
    "        \"\"\"\n",
    "        print(\"Begin to process {} stocks...\".format(str(len(self.df))))\n",
    "\n",
    "    def DigestNews(self):\n",
    "        \"\"\" Generate a NLP object and convert NEWS into categorical or numeric variables \"\"\"\n",
    "        tmp = NLP(self.df)\n",
    "        \"\"\" This class should be able to return a dataframe using a method named 'helper' for now \"\"\"\n",
    "        return tmp.helper(self.df)\n",
    "    \n",
    "    def CustomMetrics(self):\n",
    "        \"\"\" create some simple metrics \"\"\"\n",
    "        self.df.loc['pct_chg'] = self.df.close / self.df.Open - 1\n",
    "        ## MA\n",
    "        for n in [14, 30, 50, 200]:\n",
    "            self.df.loc['ma' + str(n)] = talib.SMA\n",
    "        ## RSI\n",
    "\n",
    "\n",
    "\n",
    "    def _get_max_min(self, df, MA = 1, window_range = 4, greatest = False):\n",
    "        \"\"\" return max and min values and index \"\"\"\n",
    "        df_ma = df.close.rolling(window = MA).mean().dropna()## take the Moving Average value to analyze\n",
    "        local_max = argrelextrema(df_ma.values, np.greater)[0]## get the index with greater values\n",
    "        local_min = argrelextrema(df_ma.values, np.less)[0]   ## get the index with smaller values\n",
    "        if greatest:\n",
    "            return df.iloc[local_max], df.iloc[local_min]## For triangle trend analyze, we only need greater values\n",
    "\n",
    "        ## Select the spike and the valley    \n",
    "        price_local_max_dt = []\n",
    "        for i in local_max:\n",
    "            if (i > window_range) and (i < len(df_ma) - window_range):\n",
    "                price_local_max_dt.append(df.iloc[i-window_range: i+window_range]['close'].idxmax())## 得到一定范围内值最大的位置\n",
    "        price_local_min_dt = []\n",
    "        for i in local_min:\n",
    "            if (i > window_range) and (i < len(df_ma) - window_range):\n",
    "                price_local_min_dt.append(df.iloc[i-window_range: i+window_range]['close'].idxmin())## 得到一定范围内值最小的位置\n",
    "        maxima = pd.DataFrame(df.loc[price_local_max_dt])\n",
    "        minima = pd.DataFrame(df.loc[price_local_min_dt])\n",
    "        return maxima, minima\n",
    "\n",
    "\n",
    "\n",
    "    def TriangleTrend(self,  MA=1, window_range =4, greatest = True):\n",
    "        \"\"\" a Converging price range \"\"\" \n",
    "        #1 slice dataframe into multiple parts\n",
    "        sliced_date = self.df.resample('M', level = 'date').mean().index.values\n",
    "        out = pd.DataFrame()\n",
    "        for d in sliced_date:\n",
    "            X = self.df.xs(slice(d - pd.Timedelta('30 days'), d), level = 'date', drop_level = False )\n",
    "            higher, lower = self._get_max_min(X, MA, window_range, greatest)\n",
    "            spikes, downs = higher.close.reset_index(), lower.close.reset_index()\n",
    "        ### Meethod without self.get_max_min() to get spikes and downs\n",
    "        #spikes = [[0,b[0]],[len(b)-1,b[-1]]]\n",
    "        #downs  = [[0,b[0]],[len(b)-1,b[-1]]]\n",
    "        #for i in range(1,len(b)-1):\n",
    "        #    if b[i] > b[i-1] and b[i]>b[i+1]:\n",
    "        #        spikes.append((i,b[i]))\n",
    "        #    elif b[i] < b[i-1] and b[i] < b[i+1]:\n",
    "        #        downs.append((i,b[i]))\"\"\"\n",
    "        #return spikes, downs\n",
    "        \n",
    "            res = []\n",
    "            for i in [spikes, downs]:\n",
    "                X,y = pd.DataFrame(i.date), pd.DataFrame(i.close)\n",
    "                model = LinearRegression().fit(X,y)\n",
    "                res.append(model.coef_[0][0])\n",
    "            if (res[0] - res[1])/np.mean(res) < 0.05:\n",
    "                X['TriangleTrend'] =  0# symmetrical\n",
    "            else:\n",
    "                val = np.mean(res)\n",
    "                X['TriangleTrend'] =-np.sign(val) * np.log(abs(val))\n",
    "                if val <0:\n",
    "                    print('It might keep descending after {}'.format(str(d)))\n",
    "                else:\n",
    "                    print(\"It is going up after {}\".format(str(d)))        \n",
    "            out = pd.concat([out, X])\n",
    "        out = out.set_index('date')\n",
    "        self.df = self.df.reset_index().set_index('date') \\\n",
    "                        .join(out) \\\n",
    "                        .interpolate() \\\n",
    "                        .fillna(method = 'bfill')\n",
    "        return self.df\n",
    "\n",
    "    def HeadAndShoulders(self):\n",
    "        \"\"\" Predicts a bullish-to-bearish trend reversal \"\"\"\n",
    "        sliced_date = self.df.resample('3M', level = 'date').mean().index.values\n",
    "        out = pd.DataFrame()\n",
    "        for d in sliced_date:\n",
    "            X = self.df.xs(slice(d - pd.Timedelta('93 days'), d), level = 'date', drop_level = False )\n",
    "            patterns = self._findPatterns(df = X).fillna(method = 'ffill')\n",
    "            if patterns.__len__() == 0:\n",
    "                print('No patterns detected.')\n",
    "                continue\n",
    "            try:\n",
    "                bull, bear = patterns.bull.iloc[-1], patterns.bear.iloc[-1]\n",
    "                if bull[0] > bear[0]:\n",
    "                    X['HS'] = 1\n",
    "                else:\n",
    "                    X['HS'] = -1\n",
    "                out = pd.concat([out,X])\n",
    "            except AttributeError:\n",
    "                X['HS'] = 1 if patterns.columns[0] == 'bull' else -1\n",
    "        return out\n",
    "        \n",
    "\n",
    "    \n",
    "    def _findPatterns(self, df):\n",
    "        a,b = _get_max_min(df = df, MA = 3, window_range = 4)\n",
    "        #\n",
    "        max_min = pd.concat([a,b]).close.reset_index().set_index('date').sort_index()\n",
    "        patterns = defaultdict(list)\n",
    "        \n",
    "        for i in range(5, len(max_min)):\n",
    "            window = max_min.iloc[i-5: i]\n",
    "\n",
    "            if (window.index[-1] - window.index[0]).days >100:\n",
    "                print('buxing')\n",
    "                continue## If the days difference is greater than 100 days, it might be too much, then we pass this one\n",
    "            a,b,c,d,e= window.iloc[0].close,window.iloc[1].close,window.iloc[2].close,window.iloc[3].close,window.iloc[4].close\n",
    "\n",
    "            if a<b and c<a and c<e and c<d and e<d and abs(b-d)<=np.mean([b,d])*0.02:\n",
    "                patterns['IHS'].append((window.index[0], window.index[-1]))## this one is basically abandoned\n",
    "            if (c<a or c<b) and (c<e or c<d) and (abs(a-e)/np.mean([a,e]) <= 0.02 or abs(a-d)/np.mean([a,d]) <= 0.02 or abs(b-e)/np.mean([b,e]) <= 0.02):\n",
    "                #print('bull-->{}'.format([a,b,c,d,e]))\n",
    "                patterns['bull'].append((window.index[0], window.index[-1]))\n",
    "            if (c>a or c>b) and (c>e or c>d) and (abs(a-e)/np.mean([a,e]) <= 0.02 or abs(a-d)/np.mean([a,d]) <= 0.02 or abs(b-e)/np.mean([b,e]) <= 0.02):\n",
    "                #print('bear-->{}'.format([a,b,c,d,e]))\n",
    "                patterns['bear'].append((window.index[0], window.index[-1]))\n",
    "        res = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in patterns.items()]))\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "class GetData(PipeLine):\n",
    "    def __init__(self):\n",
    "        self.df = None# This is the original data with OHLC and Volume\n",
    "        self.df2 = None#This is processed data, handle it with care\n",
    "\n",
    "    def _quandlData(self, symbols, begin_date = None, end_date = None, api_key = \"fsRzwWgnx-zG8H2L9ya2\"):\n",
    "        os.environ['QUANDL_API_KEY']  = api_key\n",
    "        out = pd.DataFrame()\n",
    "        data_source = 'quandl'\n",
    "        for symbol in symbols:\n",
    "            df = web.DataReader(symbol, data_source, begin_date, end_date)[['AdjOpen', 'AdjHigh', 'AdjLow', 'AdjClose', 'AdjVolume']].reset_index()\n",
    "            df.columns = ['date','open','high','low','close','volume']\n",
    "            df['symbol'] = symbol\n",
    "            df = df.set_index(['date', 'symbol'])\n",
    "            out = pd.concat([out,df], axis = 0)\n",
    "        self.df = out.sort_index()\n",
    "        self.df2 = self.df.copy()\n",
    "        return self.df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pipelineeeeee.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
