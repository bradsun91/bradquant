{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating getting the S&P 500 list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pythonprogramming.net/combining-stock-prices-into-one-dataframe-python-programming-for-finance/?completed=/sp500-company-price-data-python-programming-for-finance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "def save_sp500_tickers():\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17'}\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies',\n",
    "                        headers=headers)\n",
    "    \n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text\n",
    "        tickers.append(ticker)\n",
    "        \n",
    "    with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers,f)\n",
    "\n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MMM',\n",
       " 'ABT',\n",
       " 'ABBV',\n",
       " 'ACN',\n",
       " 'ATVI',\n",
       " 'AYI',\n",
       " 'ADBE',\n",
       " 'AMD',\n",
       " 'AAP',\n",
       " 'AES',\n",
       " 'AET',\n",
       " 'AMG',\n",
       " 'AFL',\n",
       " 'A',\n",
       " 'APD',\n",
       " 'AKAM',\n",
       " 'ALK',\n",
       " 'ALB',\n",
       " 'ARE',\n",
       " 'ALXN',\n",
       " 'ALGN',\n",
       " 'ALLE',\n",
       " 'AGN',\n",
       " 'ADS',\n",
       " 'LNT',\n",
       " 'ALL',\n",
       " 'GOOGL',\n",
       " 'GOOG',\n",
       " 'MO',\n",
       " 'AMZN',\n",
       " 'AEE',\n",
       " 'AAL',\n",
       " 'AEP',\n",
       " 'AXP',\n",
       " 'AIG',\n",
       " 'AMT',\n",
       " 'AWK',\n",
       " 'AMP',\n",
       " 'ABC',\n",
       " 'AME',\n",
       " 'AMGN',\n",
       " 'APH',\n",
       " 'APC',\n",
       " 'ADI',\n",
       " 'ANSS',\n",
       " 'ANTM',\n",
       " 'AON',\n",
       " 'APA',\n",
       " 'AIV',\n",
       " 'AAPL',\n",
       " 'AMAT',\n",
       " 'ADM',\n",
       " 'ARNC',\n",
       " 'AJG',\n",
       " 'AIZ',\n",
       " 'T',\n",
       " 'ADSK',\n",
       " 'ADP',\n",
       " 'AN',\n",
       " 'AZO',\n",
       " 'AVB',\n",
       " 'AVY',\n",
       " 'BHI',\n",
       " 'BLL',\n",
       " 'BAC',\n",
       " 'BK',\n",
       " 'BCR',\n",
       " 'BAX',\n",
       " 'BBT',\n",
       " 'BDX',\n",
       " 'BBBY',\n",
       " 'BRK.B',\n",
       " 'BBY',\n",
       " 'BIIB',\n",
       " 'BLK',\n",
       " 'HRB',\n",
       " 'BA',\n",
       " 'BWA',\n",
       " 'BXP',\n",
       " 'BSX',\n",
       " 'BMY',\n",
       " 'AVGO',\n",
       " 'BF.B',\n",
       " 'CHRW',\n",
       " 'CA',\n",
       " 'COG',\n",
       " 'CPB',\n",
       " 'COF',\n",
       " 'CAH',\n",
       " 'CBOE',\n",
       " 'KMX',\n",
       " 'CCL',\n",
       " 'CAT',\n",
       " 'CBG',\n",
       " 'CBS',\n",
       " 'CELG',\n",
       " 'CNC',\n",
       " 'CNP',\n",
       " 'CTL',\n",
       " 'CERN',\n",
       " 'CF',\n",
       " 'SCHW',\n",
       " 'CHTR',\n",
       " 'CHK',\n",
       " 'CVX',\n",
       " 'CMG',\n",
       " 'CB',\n",
       " 'CHD',\n",
       " 'CI',\n",
       " 'XEC',\n",
       " 'CINF',\n",
       " 'CTAS',\n",
       " 'CSCO',\n",
       " 'C',\n",
       " 'CFG',\n",
       " 'CTXS',\n",
       " 'CLX',\n",
       " 'CME',\n",
       " 'CMS',\n",
       " 'COH',\n",
       " 'KO',\n",
       " 'CTSH',\n",
       " 'CL',\n",
       " 'CMCSA',\n",
       " 'CMA',\n",
       " 'CAG',\n",
       " 'CXO',\n",
       " 'COP',\n",
       " 'ED',\n",
       " 'STZ',\n",
       " 'COO',\n",
       " 'GLW',\n",
       " 'COST',\n",
       " 'COTY',\n",
       " 'CCI',\n",
       " 'CSRA',\n",
       " 'CSX',\n",
       " 'CMI',\n",
       " 'CVS',\n",
       " 'DHI',\n",
       " 'DHR',\n",
       " 'DRI',\n",
       " 'DVA',\n",
       " 'DE',\n",
       " 'DLPH',\n",
       " 'DAL',\n",
       " 'XRAY',\n",
       " 'DVN',\n",
       " 'DLR',\n",
       " 'DFS',\n",
       " 'DISCA',\n",
       " 'DISCK',\n",
       " 'DISH',\n",
       " 'DG',\n",
       " 'DLTR',\n",
       " 'D',\n",
       " 'DOV',\n",
       " 'DOW',\n",
       " 'DPS',\n",
       " 'DTE',\n",
       " 'DD',\n",
       " 'DUK',\n",
       " 'DXC',\n",
       " 'ETFC',\n",
       " 'EMN',\n",
       " 'ETN',\n",
       " 'EBAY',\n",
       " 'ECL',\n",
       " 'EIX',\n",
       " 'EW',\n",
       " 'EA',\n",
       " 'EMR',\n",
       " 'ETR',\n",
       " 'EVHC',\n",
       " 'EOG',\n",
       " 'EQT',\n",
       " 'EFX',\n",
       " 'EQIX',\n",
       " 'EQR',\n",
       " 'ESS',\n",
       " 'EL',\n",
       " 'ES',\n",
       " 'RE',\n",
       " 'EXC',\n",
       " 'EXPE',\n",
       " 'EXPD',\n",
       " 'ESRX',\n",
       " 'EXR',\n",
       " 'XOM',\n",
       " 'FFIV',\n",
       " 'FB',\n",
       " 'FAST',\n",
       " 'FRT',\n",
       " 'FDX',\n",
       " 'FIS',\n",
       " 'FITB',\n",
       " 'FE',\n",
       " 'FISV',\n",
       " 'FLIR',\n",
       " 'FLS',\n",
       " 'FLR',\n",
       " 'FMC',\n",
       " 'FL',\n",
       " 'F',\n",
       " 'FTV',\n",
       " 'FBHS',\n",
       " 'BEN',\n",
       " 'FCX',\n",
       " 'GPS',\n",
       " 'GRMN',\n",
       " 'IT',\n",
       " 'GD',\n",
       " 'GE',\n",
       " 'GGP',\n",
       " 'GIS',\n",
       " 'GM',\n",
       " 'GPC',\n",
       " 'GILD',\n",
       " 'GPN',\n",
       " 'GS',\n",
       " 'GT',\n",
       " 'GWW',\n",
       " 'HAL',\n",
       " 'HBI',\n",
       " 'HOG',\n",
       " 'HRS',\n",
       " 'HIG',\n",
       " 'HAS',\n",
       " 'HCA',\n",
       " 'HCP',\n",
       " 'HP',\n",
       " 'HSIC',\n",
       " 'HSY',\n",
       " 'HES',\n",
       " 'HPE',\n",
       " 'HLT',\n",
       " 'HOLX',\n",
       " 'HD',\n",
       " 'HON',\n",
       " 'HRL',\n",
       " 'HST',\n",
       " 'HPQ',\n",
       " 'HUM',\n",
       " 'HBAN',\n",
       " 'IDXX',\n",
       " 'INFO',\n",
       " 'ITW',\n",
       " 'ILMN',\n",
       " 'IR',\n",
       " 'INTC',\n",
       " 'ICE',\n",
       " 'IBM',\n",
       " 'INCY',\n",
       " 'IP',\n",
       " 'IPG',\n",
       " 'IFF',\n",
       " 'INTU',\n",
       " 'ISRG',\n",
       " 'IVZ',\n",
       " 'IRM',\n",
       " 'JEC',\n",
       " 'JBHT',\n",
       " 'SJM',\n",
       " 'JNJ',\n",
       " 'JCI',\n",
       " 'JPM',\n",
       " 'JNPR',\n",
       " 'KSU',\n",
       " 'K',\n",
       " 'KEY',\n",
       " 'KMB',\n",
       " 'KIM',\n",
       " 'KMI',\n",
       " 'KLAC',\n",
       " 'KSS',\n",
       " 'KHC',\n",
       " 'KR',\n",
       " 'LB',\n",
       " 'LLL',\n",
       " 'LH',\n",
       " 'LRCX',\n",
       " 'LEG',\n",
       " 'LEN',\n",
       " 'LVLT',\n",
       " 'LUK',\n",
       " 'LLY',\n",
       " 'LNC',\n",
       " 'LKQ',\n",
       " 'LMT',\n",
       " 'L',\n",
       " 'LOW',\n",
       " 'LYB',\n",
       " 'MTB',\n",
       " 'MAC',\n",
       " 'M',\n",
       " 'MNK',\n",
       " 'MRO',\n",
       " 'MPC',\n",
       " 'MAR',\n",
       " 'MMC',\n",
       " 'MLM',\n",
       " 'MAS',\n",
       " 'MA',\n",
       " 'MAT',\n",
       " 'MKC',\n",
       " 'MCD',\n",
       " 'MCK',\n",
       " 'MDT',\n",
       " 'MRK',\n",
       " 'MET',\n",
       " 'MTD',\n",
       " 'KORS',\n",
       " 'MCHP',\n",
       " 'MU',\n",
       " 'MSFT',\n",
       " 'MAA',\n",
       " 'MHK',\n",
       " 'TAP',\n",
       " 'MDLZ',\n",
       " 'MON',\n",
       " 'MNST',\n",
       " 'MCO',\n",
       " 'MS',\n",
       " 'MOS',\n",
       " 'MSI',\n",
       " 'MUR',\n",
       " 'MYL',\n",
       " 'NDAQ',\n",
       " 'NOV',\n",
       " 'NAVI',\n",
       " 'NTAP',\n",
       " 'NFLX',\n",
       " 'NWL',\n",
       " 'NFX',\n",
       " 'NEM',\n",
       " 'NWSA',\n",
       " 'NWS',\n",
       " 'NEE',\n",
       " 'NLSN',\n",
       " 'NKE',\n",
       " 'NI',\n",
       " 'NBL',\n",
       " 'JWN',\n",
       " 'NSC',\n",
       " 'NTRS',\n",
       " 'NOC',\n",
       " 'NRG',\n",
       " 'NUE',\n",
       " 'NVDA',\n",
       " 'ORLY',\n",
       " 'OXY',\n",
       " 'OMC',\n",
       " 'OKE',\n",
       " 'ORCL',\n",
       " 'PCAR',\n",
       " 'PH',\n",
       " 'PDCO',\n",
       " 'PAYX',\n",
       " 'PYPL',\n",
       " 'PNR',\n",
       " 'PBCT',\n",
       " 'PEP',\n",
       " 'PKI',\n",
       " 'PRGO',\n",
       " 'PFE',\n",
       " 'PCG',\n",
       " 'PM',\n",
       " 'PSX',\n",
       " 'PNW',\n",
       " 'PXD',\n",
       " 'PNC',\n",
       " 'RL',\n",
       " 'PPG',\n",
       " 'PPL',\n",
       " 'PX',\n",
       " 'PCLN',\n",
       " 'PFG',\n",
       " 'PG',\n",
       " 'PGR',\n",
       " 'PLD',\n",
       " 'PRU',\n",
       " 'PEG',\n",
       " 'PSA',\n",
       " 'PHM',\n",
       " 'PVH',\n",
       " 'QRVO',\n",
       " 'PWR',\n",
       " 'QCOM',\n",
       " 'DGX',\n",
       " 'RRC',\n",
       " 'RJF',\n",
       " 'RTN',\n",
       " 'O',\n",
       " 'RHT',\n",
       " 'REG',\n",
       " 'REGN',\n",
       " 'RF',\n",
       " 'RSG',\n",
       " 'RAI',\n",
       " 'RHI',\n",
       " 'ROK',\n",
       " 'COL',\n",
       " 'ROP',\n",
       " 'ROST',\n",
       " 'RCL',\n",
       " 'CRM',\n",
       " 'SCG',\n",
       " 'SLB',\n",
       " 'SNI',\n",
       " 'STX',\n",
       " 'SEE',\n",
       " 'SRE',\n",
       " 'SHW',\n",
       " 'SIG',\n",
       " 'SPG',\n",
       " 'SWKS',\n",
       " 'SLG',\n",
       " 'SNA',\n",
       " 'SO',\n",
       " 'LUV',\n",
       " 'SPGI',\n",
       " 'SWK',\n",
       " 'SPLS',\n",
       " 'SBUX',\n",
       " 'STT',\n",
       " 'SRCL',\n",
       " 'SYK',\n",
       " 'STI',\n",
       " 'SYMC',\n",
       " 'SYF',\n",
       " 'SNPS',\n",
       " 'SYY',\n",
       " 'TROW',\n",
       " 'TGT',\n",
       " 'TEL',\n",
       " 'FTI',\n",
       " 'TSO',\n",
       " 'TXN',\n",
       " 'TXT',\n",
       " 'TMO',\n",
       " 'TIF',\n",
       " 'TWX',\n",
       " 'TJX',\n",
       " 'TMK',\n",
       " 'TSS',\n",
       " 'TSCO',\n",
       " 'TDG',\n",
       " 'RIG',\n",
       " 'TRV',\n",
       " 'TRIP',\n",
       " 'FOXA',\n",
       " 'FOX',\n",
       " 'TSN',\n",
       " 'UDR',\n",
       " 'ULTA',\n",
       " 'USB',\n",
       " 'UA',\n",
       " 'UAA',\n",
       " 'UNP',\n",
       " 'UAL',\n",
       " 'UNH',\n",
       " 'UPS',\n",
       " 'URI',\n",
       " 'UTX',\n",
       " 'UHS',\n",
       " 'UNM',\n",
       " 'VFC',\n",
       " 'VLO',\n",
       " 'VAR',\n",
       " 'VTR',\n",
       " 'VRSN',\n",
       " 'VRSK',\n",
       " 'VZ',\n",
       " 'VRTX',\n",
       " 'VIAB',\n",
       " 'V',\n",
       " 'VNO',\n",
       " 'VMC',\n",
       " 'WMT',\n",
       " 'WBA',\n",
       " 'DIS',\n",
       " 'WM',\n",
       " 'WAT',\n",
       " 'WEC',\n",
       " 'WFC',\n",
       " 'HCN',\n",
       " 'WDC',\n",
       " 'WU',\n",
       " 'WRK',\n",
       " 'WY',\n",
       " 'WHR',\n",
       " 'WFM',\n",
       " 'WMB',\n",
       " 'WLTW',\n",
       " 'WYN',\n",
       " 'WYNN',\n",
       " 'XEL',\n",
       " 'XRX',\n",
       " 'XLNX',\n",
       " 'XL',\n",
       " 'XYL',\n",
       " 'YUM',\n",
       " 'ZBH',\n",
       " 'ZION',\n",
       " 'ZTS']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_sp500_tickers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting all company pricing data in the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this part is good:\n",
    "\n",
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "def save_sp500_tickers():\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text\n",
    "        tickers.append(ticker)\n",
    "        \n",
    "    with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers,f)\n",
    "        \n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to add a few new imports:\n",
    "\n",
    "import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_from_yahoo(reload_sp500=False):\n",
    "    \n",
    "    if reload_sp500:\n",
    "        tickers = save_sp500_tickers()\n",
    "    else:\n",
    "        with open(\"sp500tickers.pickle\",\"rb\") as f:\n",
    "            tickers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ne need to decide what we're going to do with the data. What I tend to do is try to parse websites ONCE, and store the data locally. I don't try to know in advance all of the things I might do with the data, but I know if I am going to pull it more than once, I might as well just save it (unless it's a huge dataset, which this is not). Thus, we're going to pull everything we can from what Yahoo returns to us for every stock and just save it. To do this, we'll create a new directory, and, in there, store stock data per company. To begin, we need that initial directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could just store these datasets in the same directory as your script, but this would get pretty messy in my opinion. Now we're ready to pull the data. You already know how to do this, we did it in the very first tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_from_google(reload_sp500=False):\n",
    "    \n",
    "    if reload_sp500:\n",
    "        tickers = save_sp500_tickers()\n",
    "    else:\n",
    "        with open(\"sp500tickers.pickle\",\"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "\n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "\n",
    "    start = dt.datetime(2000, 1, 1)\n",
    "    end = dt.datetime(2016, 12, 31)\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            df = web.DataReader(ticker, \"google\", start, end)\n",
    "            df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will likely in time want to do some sort of force_data_update parameter to this function, since, right now, it will not re-pull data it already sees hit has. Since we're pulling daily data, you'd want to have this re-pulling at least the latest data. That said, if that's the case, you might be better off with using a database instead with a table per company, and then just pulling the most recent values from the Yahoo database. We'll keep things simple for now though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already have MMM\n",
      "Already have ABT\n",
      "Already have ABBV\n",
      "Already have ACN\n",
      "Already have ATVI\n",
      "Already have AYI\n",
      "Already have ADBE\n",
      "Already have AMD\n",
      "Already have AAP\n",
      "Already have AES\n",
      "Already have AET\n",
      "Already have AMG\n",
      "Already have AFL\n",
      "Already have A\n",
      "Already have APD\n",
      "Already have AKAM\n",
      "Already have ALK\n",
      "Already have ALB\n",
      "Already have ARE\n",
      "Already have ALXN\n",
      "Already have ALGN\n",
      "Already have ALLE\n",
      "Already have AGN\n",
      "Already have ADS\n",
      "Already have LNT\n",
      "Already have ALL\n",
      "Already have GOOGL\n",
      "Already have GOOG\n",
      "Already have MO\n",
      "Already have AMZN\n",
      "Already have AEE\n",
      "Already have AAL\n",
      "Already have AEP\n",
      "Already have AXP\n",
      "Already have AIG\n",
      "Already have AMT\n",
      "Already have AWK\n",
      "Already have AMP\n",
      "Already have ABC\n",
      "Already have AME\n",
      "Already have AMGN\n",
      "Already have APH\n",
      "Already have APC\n",
      "Already have ADI\n",
      "Already have ANSS\n",
      "Already have ANTM\n",
      "Already have AON\n",
      "Already have APA\n",
      "Already have AIV\n",
      "Already have AAPL\n",
      "Already have AMAT\n",
      "Already have ADM\n",
      "Already have ARNC\n",
      "Already have AJG\n",
      "Already have AIZ\n",
      "Already have T\n",
      "Already have ADSK\n",
      "Already have ADP\n",
      "Already have AN\n",
      "Already have AZO\n",
      "Already have AVB\n",
      "Already have AVY\n",
      "Already have BHI\n",
      "Already have BLL\n",
      "Already have BAC\n",
      "Already have BK\n",
      "Already have BCR\n",
      "Already have BAX\n",
      "Already have BBT\n",
      "Already have BDX\n",
      "Already have BBBY\n",
      "Already have BRK.B\n",
      "Already have BBY\n",
      "Already have BIIB\n",
      "Already have BLK\n",
      "Already have HRB\n",
      "Already have BA\n",
      "Already have BWA\n",
      "Already have BXP\n",
      "Already have BSX\n",
      "Already have BMY\n",
      "Already have AVGO\n",
      "Already have BF.B\n",
      "Already have CHRW\n",
      "Already have CA\n",
      "Already have COG\n",
      "Already have CPB\n",
      "Already have COF\n",
      "Already have CAH\n",
      "Already have CBOE\n",
      "Already have KMX\n",
      "Already have CCL\n",
      "Already have CAT\n",
      "Already have CBG\n",
      "Already have CBS\n",
      "Already have CELG\n",
      "Already have CNC\n",
      "Already have CNP\n",
      "Already have CTL\n",
      "Already have CERN\n",
      "Already have CF\n",
      "Already have SCHW\n",
      "Already have CHTR\n",
      "Already have CHK\n",
      "Already have CVX\n",
      "Already have CMG\n",
      "Already have CB\n",
      "Already have CHD\n",
      "Already have CI\n",
      "Already have XEC\n",
      "Already have CINF\n",
      "Already have CTAS\n",
      "Already have CSCO\n",
      "Already have C\n",
      "Already have CFG\n",
      "Already have CTXS\n",
      "Already have CLX\n",
      "Already have CME\n",
      "Already have CMS\n",
      "Already have COH\n",
      "Already have KO\n",
      "Already have CTSH\n",
      "Already have CL\n",
      "Already have CMCSA\n",
      "Already have CMA\n",
      "Already have CAG\n",
      "Already have CXO\n",
      "Already have COP\n",
      "Already have ED\n",
      "Already have STZ\n",
      "Already have COO\n",
      "Already have GLW\n",
      "Already have COST\n",
      "Already have COTY\n",
      "Already have CCI\n",
      "Already have CSRA\n",
      "Already have CSX\n",
      "Already have CMI\n",
      "Already have CVS\n",
      "Already have DHI\n",
      "Already have DHR\n",
      "Already have DRI\n",
      "Already have DVA\n",
      "Already have DE\n",
      "Already have DLPH\n",
      "Already have DAL\n",
      "Already have XRAY\n",
      "Already have DVN\n",
      "Already have DLR\n",
      "Already have DFS\n",
      "Already have DISCA\n",
      "Already have DISCK\n",
      "Already have DISH\n",
      "Already have DG\n",
      "Already have DLTR\n",
      "Already have D\n",
      "Already have DOV\n",
      "Already have DOW\n",
      "Already have DPS\n",
      "Already have DTE\n",
      "Already have DD\n",
      "Already have DUK\n",
      "Already have DXC\n",
      "Already have ETFC\n",
      "Already have EMN\n",
      "Already have ETN\n",
      "Already have EBAY\n",
      "Already have ECL\n",
      "Already have EIX\n",
      "Already have EW\n",
      "Already have EA\n",
      "Already have EMR\n",
      "Already have ETR\n",
      "Already have EVHC\n",
      "Already have EOG\n",
      "Already have EQT\n",
      "Already have EFX\n",
      "Already have EQIX\n",
      "Already have EQR\n",
      "Already have ESS\n",
      "Already have EL\n",
      "Already have ES\n",
      "Already have RE\n",
      "Already have EXC\n",
      "Already have EXPE\n",
      "Already have EXPD\n",
      "Already have ESRX\n",
      "Already have EXR\n",
      "Already have XOM\n",
      "Already have FFIV\n",
      "Already have FB\n",
      "Already have FAST\n",
      "Already have FRT\n",
      "Already have FDX\n",
      "Already have FIS\n",
      "Already have FITB\n",
      "Already have FE\n",
      "Already have FISV\n",
      "Already have FLIR\n",
      "Already have FLS\n",
      "Already have FLR\n",
      "Already have FMC\n",
      "Already have FL\n",
      "Already have F\n",
      "Already have FTV\n",
      "Already have FBHS\n",
      "Already have BEN\n",
      "Already have FCX\n",
      "Already have GPS\n",
      "Already have GRMN\n",
      "Already have IT\n",
      "Already have GD\n",
      "Already have GE\n",
      "Already have GGP\n",
      "Already have GIS\n",
      "Already have GM\n",
      "Already have GPC\n",
      "Already have GILD\n",
      "Already have GPN\n",
      "Already have GS\n",
      "Already have GT\n",
      "Already have GWW\n",
      "Already have HAL\n",
      "Already have HBI\n",
      "Already have HOG\n",
      "Already have HRS\n",
      "Already have HIG\n",
      "Already have HAS\n",
      "Already have HCA\n",
      "Already have HCP\n",
      "Already have HP\n",
      "Already have HSIC\n",
      "Already have HSY\n",
      "Already have HES\n",
      "Already have HPE\n",
      "Already have HLT\n",
      "Already have HOLX\n",
      "Already have HD\n",
      "Already have HON\n",
      "Already have HRL\n",
      "Already have HST\n",
      "Already have HPQ\n",
      "Already have HUM\n",
      "Already have HBAN\n",
      "Already have IDXX\n",
      "Already have INFO\n",
      "Already have ITW\n",
      "Already have ILMN\n",
      "Already have IR\n",
      "Already have INTC\n",
      "Already have ICE\n",
      "Already have IBM\n",
      "Already have INCY\n",
      "Already have IP\n",
      "Already have IPG\n",
      "Already have IFF\n",
      "Already have INTU\n",
      "Already have ISRG\n",
      "Already have IVZ\n",
      "Already have IRM\n",
      "Already have JEC\n",
      "Already have JBHT\n",
      "Already have SJM\n",
      "Already have JNJ\n",
      "Already have JCI\n",
      "Already have JPM\n",
      "Already have JNPR\n",
      "Already have KSU\n",
      "Already have K\n",
      "Already have KEY\n",
      "Already have KMB\n",
      "Already have KIM\n",
      "Already have KMI\n",
      "Already have KLAC\n",
      "Already have KSS\n",
      "Already have KHC\n",
      "Already have KR\n",
      "Already have LB\n",
      "Already have LLL\n",
      "Already have LH\n",
      "Already have LRCX\n",
      "Already have LEG\n",
      "Already have LEN\n",
      "Already have LVLT\n",
      "Already have LUK\n",
      "Already have LLY\n",
      "Already have LNC\n",
      "Already have LKQ\n"
     ]
    },
    {
     "ename": "RemoteDataError",
     "evalue": "Unable to read URL: http://www.google.com/finance/historical",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDataError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-601ac81e27e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_data_from_google\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-8770aa3354a6>\u001b[0m in \u001b[0;36mget_data_from_google\u001b[0;34m(reload_sp500)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtickers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stock_dfs/{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"google\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stock_dfs/{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\pandas_datareader\\data.py\u001b[0m in \u001b[0;36mDataReader\u001b[0;34m(name, data_source, start, end, retry_count, pause, session)\u001b[0m\n\u001b[1;32m    103\u001b[0m                                  \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                                  \u001b[0mretry_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretry_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpause\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpause\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                                  session=session).read()\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdata_source\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"fred\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[1;31m# If a single symbol, (e.g., 'GOOG')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_one_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[1;31m# Or multiple symbols, (e.g., ['GOOG', 'AAPL', 'MSFT'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py\u001b[0m in \u001b[0;36m_read_one_data\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[1;34m\"\"\" read one data from specified URL \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'string'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_url_as_StringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'json'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py\u001b[0m in \u001b[0;36m_read_url_as_StringIO\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mOpen\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mand\u001b[0m \u001b[0mretry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mRemoteDataError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unable to read URL: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRemoteDataError\u001b[0m: Unable to read URL: http://www.google.com/finance/historical"
     ]
    }
   ],
   "source": [
    "get_data_from_google()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining all S&P 500 company prices into one DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we do have all of the data at our disposal, we may want to actually assess the data together. To do this, we're going to join all of the stock datasets together. Each of the stock files at the moment come with: Open, High, Low, Close, Volume, and Adj Close. At least to start, we're mostly just interested in the adjusted close for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'stock_dfs/LMT.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e501aefa12d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mcompile_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-e501aefa12d8>\u001b[0m in \u001b[0;36mcompile_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mticker\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stock_dfs/{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Brad Sun\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:4019)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:7967)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'stock_dfs/LMT.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "\n",
    "def save_sp500_tickers():\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text\n",
    "        tickers.append(ticker)\n",
    "        \n",
    "    with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers,f)\n",
    "        \n",
    "    return tickers\n",
    "\n",
    "\n",
    "def get_data_from_yahoo(reload_sp500=False):\n",
    "    \n",
    "    if reload_sp500:\n",
    "        tickers = save_sp500_tickers()\n",
    "    else:\n",
    "        with open(\"sp500tickers.pickle\",\"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    \n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2000, 1, 1)\n",
    "    end = dt.datetime(2016, 12, 31)\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        # just in case your connection breaks, we'd like to save our progress!\n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            df = web.DataReader(ticker, \"yahoo\", start, end)\n",
    "            df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "\n",
    "\n",
    "def compile_data():\n",
    "    with open(\"sp500tickers.pickle\",\"rb\") as f:\n",
    "        tickers = pickle.load(f)\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "    \n",
    "    for count,ticker in enumerate(tickers):\n",
    "        df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        df.rename(columns={'Adj Close':ticker}, inplace=True)\n",
    "        df.drop(['Open','High','Low','Close','Volume'],1,inplace=True)\n",
    "\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df, how='outer')\n",
    "\n",
    "        if count % 10 == 0:\n",
    "            print(count)\n",
    "    print(main_df.head())\n",
    "    main_df.to_csv('sp500_joined_closes.csv')\n",
    "\n",
    "\n",
    "compile_data()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
